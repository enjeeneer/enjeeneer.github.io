<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects on Scott Jeen</title>
    <link>https://enjeeneer.io/projects/</link>
    <description>Recent content in Projects on Scott Jeen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Tue, 26 Sep 2023 17:27:21 +0100</lastBuildDate><atom:link href="https://enjeeneer.io/projects/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Zero-Shot Reinforcement Learning from Low Quality Data</title>
      <link>https://enjeeneer.io/projects/zero-shot-rl/</link>
      <pubDate>Tue, 26 Sep 2023 17:27:21 +0100</pubDate>
      
      <guid>https://enjeeneer.io/projects/zero-shot-rl/</guid>
      <description>NeurIPS 2024 Scott Jeen\(^{1}\), Tom Bewley\(^{2}\), &amp;amp; Jonathan M. Cullen\(^{1}\) \(^{1}\) University of Cambridge
\(^{2}\) University of Bristol
[Paper] [Code] [Poster] [Slides]
 Summary Zero-shot reinforcement learning (RL) methods learn general policies that can, in principle, solve any unseen task in an environment. Recently, methods leveraging successor features and successor measures have emerged as viable zero-shot RL candidates, returning near-optimal policies for many unseen tasks. However, to enable this, they have assumed access to unrealistically large and heterogeneous datasets of transitions for pre-training.</description>
    </item>
    
    <item>
      <title>Low Emission Building Control with Zero-Shot Reinforcement Learning</title>
      <link>https://enjeeneer.io/projects/pearl/</link>
      <pubDate>Fri, 12 Aug 2022 09:34:36 +0100</pubDate>
      
      <guid>https://enjeeneer.io/projects/pearl/</guid>
      <description>In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 37, No. 12, pp. 14259-14267)
Scott R. Jeen\(^{1,3}\), Alessandro Abate\(^{2,3}\), Jonathan M. Cullen\(^{1}\) \(^{1}\) University of Cambridge
\(^{2}\) University of Oxford
\(^{3}\) Alan Turing Institute
[Paper] [Code] [Talk] [Poster]
   PEARL: Probabilistic Emission-Abating Reinforcement Learning  Presenting PEARL: Probabilistic Emission-Abating Reinforcement Learning, a deep RL algorithm that can find performant building control policies online, without pre-training&amp;ndash;the first time this has been shown to be possible.</description>
    </item>
    
  </channel>
</rss>
