<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="Scott Jeen">
<meta name="description" content="RL Conference 2025 Scott Jeen\(^{1}\), Tom Bewley, &amp;amp; Jonathan M. Cullen\(^{1}\) \(^{1}\) University of Cambridge
[Paper] [Code]
 Summary Zero-shot reinforcement learning (RL) methods learn general policies that can, in principle, solve any unseen task in an environment. Recently, methods leveraging successor features and successor measures have emerged as viable zero-shot RL candidates, returning near-optimal policies for many unseen tasks. However, to enable this, they have assumed access to unrealistically large and heterogeneous datasets of transitions for pre-training." />
<meta name="keywords" content="Scott Jeen" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="#6B8E23" />
<link rel="canonical" href="https://enjeeneer.io/projects/bfms-with-memory/" />


<script async src="https://www.googletagmanager.com/gtag/js?id=G-P95NFMWGS8"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-P95NFMWGS8', { 'anonymize_ip': false });
}
</script>



    <title>
        
            Zero-Shot Reinforcement Learning Under Partial Observability :: Scott Jeen  — AI researcher
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="/main.cbec4bded5981a9639be0a9253dbc5636fb308fba08c31a50670aee0a034a132.css">




    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#6B8E23">
    <link rel="shortcut icon" href="/favicon.ico">
    <meta name="msapplication-TileColor" content="#6B8E23">
    <meta name="theme-color" content="#6B8E23">



<meta itemprop="name" content="Zero-Shot Reinforcement Learning Under Partial Observability">
<meta itemprop="description" content="RL Conference 2025 Scott Jeen\(^{1}\), Tom Bewley, &amp; Jonathan M. Cullen\(^{1}\) \(^{1}\) University of Cambridge
[Paper] [Code]
 Summary Zero-shot reinforcement learning (RL) methods learn general policies that can, in principle, solve any unseen task in an environment. Recently, methods leveraging successor features and successor measures have emerged as viable zero-shot RL candidates, returning near-optimal policies for many unseen tasks. However, to enable this, they have assumed access to unrealistically large and heterogeneous datasets of transitions for pre-training."><meta itemprop="datePublished" content="2025-06-12T21:48:13+01:00" />
<meta itemprop="dateModified" content="2025-06-12T21:48:13+01:00" />
<meta itemprop="wordCount" content="412"><meta itemprop="image" content="https://enjeeneer.io"/>
<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://enjeeneer.io"/>

<meta name="twitter:title" content="Zero-Shot Reinforcement Learning Under Partial Observability"/>
<meta name="twitter:description" content="RL Conference 2025 Scott Jeen\(^{1}\), Tom Bewley, &amp; Jonathan M. Cullen\(^{1}\) \(^{1}\) University of Cambridge
[Paper] [Code]
 Summary Zero-shot reinforcement learning (RL) methods learn general policies that can, in principle, solve any unseen task in an environment. Recently, methods leveraging successor features and successor measures have emerged as viable zero-shot RL candidates, returning near-optimal policies for many unseen tasks. However, to enable this, they have assumed access to unrealistically large and heterogeneous datasets of transitions for pre-training."/>



    <meta property="og:title" content="Zero-Shot Reinforcement Learning Under Partial Observability" />
<meta property="og:description" content="RL Conference 2025 Scott Jeen\(^{1}\), Tom Bewley, &amp; Jonathan M. Cullen\(^{1}\) \(^{1}\) University of Cambridge
[Paper] [Code]
 Summary Zero-shot reinforcement learning (RL) methods learn general policies that can, in principle, solve any unseen task in an environment. Recently, methods leveraging successor features and successor measures have emerged as viable zero-shot RL candidates, returning near-optimal policies for many unseen tasks. However, to enable this, they have assumed access to unrealistically large and heterogeneous datasets of transitions for pre-training." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://enjeeneer.io/projects/bfms-with-memory/" /><meta property="og:image" content="https://enjeeneer.io"/><meta property="article:section" content="projects" />
<meta property="article:published_time" content="2025-06-12T21:48:13+01:00" />
<meta property="article:modified_time" content="2025-06-12T21:48:13+01:00" />







    <meta property="article:published_time" content="2025-06-12 21:48:13 &#43;0100 BST" />








<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script>

    
    </head>

    <body class="">
        <div class="container">
            <header class="header">
    
<script async src="https://www.googletagmanager.com/gtag/js?id=G-P95NFMWGS8"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-P95NFMWGS8', { 'anonymize_ip': false });
}
</script>

    <span class="header__inner">
        <a href="/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text">/enjeeneer/</span>
            <span class="logo__cursor" style=
                  "
                   background-color:#6B8E23;
                   animation-duration:1s;">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://enjeeneer.io/about/">about</a></li><li><a href="https://enjeeneer.io/posts/">posts</a></li><li><a href="https://enjeeneer.io/projects/">projects</a></li><li><a href="https://scholar.google.com/citations?user=3HPX720AAAAJ&amp;hl=en&amp;oi=ao">scholar</a></li><li><a href="https://enjeeneer.io/talks/">talks</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            

            <span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>
</span>
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            
            </p>
        </div>

        <article>
            <h2 class="post-title"><a href="https://enjeeneer.io/projects/bfms-with-memory/">Zero-Shot Reinforcement Learning Under Partial Observability</a></h2>
            <h3 class="loc"><a href="https://enjeeneer.io/projects/bfms-with-memory/"></a></h3>
            <h3 class="time"><a href="https://enjeeneer.io/projects/bfms-with-memory/"></a></h3>

            

            

            <div class="post-content">
                <h2 id="rl-conference-2025">RL Conference 2025</h2>
<h3 id="scott-jeen1-tom-bewley--jonathan-m-cullen1">Scott Jeen\(^{1}\), Tom Bewley, &amp; Jonathan M. Cullen\(^{1}\)</h3>
<p>\(^{1}\) <em>University of Cambridge</em></p>
<p><strong><a href="https://arxiv.org/abs/2309.15178">[Paper]</a></strong>
<strong><a href="https://github.com/enjeeneer/zero-shot-rl">[Code]</a></strong></p>
<hr>
<h2 id="summary">Summary</h2>
<p>Zero-shot reinforcement learning (RL) methods learn general policies that can, in principle, solve any unseen task in an environment. Recently,
methods leveraging successor features and successor measures have emerged as viable
zero-shot RL candidates, returning near-optimal policies
for many unseen tasks. However, to enable this, they have assumed access to unrealistically large and heterogeneous datasets of transitions for pre-training. Most real datasets,
like historical logs created by existing control systems, are smaller and less diverse than these current methods expect. As a result, this paper asks:</p>
<p><strong>Can we still perform zero-shot RL with small, homogeneous datasets?</strong></p>
<hr>
<h2 id="intuition">Intuition</h2>
<p>When the dataset is inexhaustive, existing methods like FB representations overestimate the value of actions not in the dataset (Figure 2). The above
shows this overestimation as dataset size and quality is varied. The smaller and less diverse the dataset, the more
\(Q\) values tend to be overestimated.</p>
<figure><img src="https://github.com/enjeeneer/conservative-world-models/blob/main/media/overestimates.png?raw=true"/>
</figure>

<p><small><em>Figure 1: <strong>FB value overestimation with respect to dataset size \(n\) and quality</strong>. \(Q\) values predicted during training
increase as both the size and “quality&quot; of the dataset decrease. This contradicts the low return of all resultant
policies (note: a return of 1000 is the maximum achievable for this task).</em> </small></p>
<p>We fix this by suppressing the predicted values (or measures) for state-actions not in the dataset.</p>
<figure><img src="https://github.com/enjeeneer/conservative-world-models/blob/main/media/vcfb-intuition-final.png?raw=true"/>
</figure>

<p><small><em>Figure 2: <strong>Conservative zero-shot RL methods</strong>. VC-FB (right) suppresses the predicted values for OOD state-action pairs**</em></small></p>
<hr>
<h2 id="results">Results</h2>
<p>We demonstrate our methods improve performance w.r.t standard zero-shot RL / GCRL baselines on low quality datasets from ExORL (Figure 3) and D4RL (Figure 4), and do not hinder performance on large and diverse datasets (Figure 5).
<figure><img src="https://github.com/enjeeneer/zero-shot-rl/blob/main/media/performance-profiles-subplot2.png?raw=true"/>
</figure>
</p>
<p><small><em>Figure 3: <strong>Aggregate zero-shot performance on ExORL.</strong> (Left) IQM of task scores across datasets and domains,
normalised against the performance of CQL, our baseline. (Right) Performance profiles showing the distribution
of scores across all tasks and domains. Both conservative FB variants stochastically dominate vanilla FB.</em></small></p>
<figure><img src="https://github.com/enjeeneer/zero-shot-rl/blob/main/media/d4rl-performance.png?raw=true" width="50%"/>
</figure>

<p><small><em>Figure 4: <strong>Aggregate zero-shot RL performance on D4RL.</strong> Aggregate IQM scores across all domains and datasets,
normalised against the performance of CQL.</em></small></p>
<figure><img src="/img/zero-shot-rl-dataset-size.png" width="450"/>
</figure>

<p><small><em>Figure 5:  <strong>Performance by RND dataset size.</strong> The performance gap between conservative FB variants and vanilla FB increases as dataset size decreases</em></small></p>
<hr>
<p>Read the <a href="https://arxiv.org/abs/2309.15178">full paper</a> for more details!</p>
<pre tabindex="0"><code class="language-commandline" data-lang="commandline">@article{jeen2025,
  url = {https://arxiv.org/abs/2309.15178},
  author = {Jeen, Scott and Bewley, Tom and Cullen, Jonathan M.},  
  title = {Zero-Shot Reinforcement Learning Under Partial Observability},
  journal = {2nd Reinforcement Learning Conference},
  year = {2025},
}
</code></pre><hr>

            </div>
        </article>

        <hr />

        <div class="post-info">
            
            
  		</div>

    </main>

            </div>

            
        </div>

        




<script type="text/javascript" src="/bundle.min.a0f363fdf81cdc5cfacc447a79c33189eb000d090336cd04aac8ee256f423b3133b836c281944c19c75e38d0b0b449f01ce5807e37798b7ac94ac1db51983fc4.js" integrity="sha512-oPNj/fgc3Fz6zER6ecMxiesADQkDNs0EqsjuJW9COzEzuDbCgZRMGcdeONCwtEnwHOWAfjd5i3rJSsHbUZg/xA=="></script>
    
<script async src="https://www.googletagmanager.com/gtag/js?id=G-P95NFMWGS8"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-P95NFMWGS8', { 'anonymize_ip': false });
}
</script>




    </body>
</html>
