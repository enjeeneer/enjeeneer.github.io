<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Scott Jeen</title>
    <link>https://enjeeneer.io/</link>
    <description>Recent content on Scott Jeen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Tue, 26 Sep 2023 17:27:21 +0100</lastBuildDate><atom:link href="https://enjeeneer.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Conservative World Models</title>
      <link>https://enjeeneer.io/projects/conservative-world-models/</link>
      <pubDate>Tue, 26 Sep 2023 17:27:21 +0100</pubDate>
      
      <guid>https://enjeeneer.io/projects/conservative-world-models/</guid>
      <description>Scott Jeen\(^{1}\), Tom Bewley\(^{2}\), &amp;amp; Jonathan M. Cullen\(^{1}\) \(^{1}\) University of Cambridge
\(^{2}\) University of Bristol
[Paper] [Code] [Poster]
 Summary Imagine you&amp;rsquo;ve collected a dataset from a system you&amp;rsquo;d like to control more efficiently. Examples include: household robots, chemical manufacturing processes, autonomous vehicles, or steel-making furnaces. An ideal solution would be to train an autonomous agent on your dataset, then for it to use what it learns to solve any task inside the system.</description>
    </item>
    
    <item>
      <title>NeurIPS 2022</title>
      <link>https://enjeeneer.io/posts/2023/01/neurips-2022/</link>
      <pubDate>Thu, 26 Jan 2023 21:08:05 +0000</pubDate>
      
      <guid>https://enjeeneer.io/posts/2023/01/neurips-2022/</guid>
      <description>I was fortunate to attend NeurIPS in New Orleans in November. Here, I publish my takeaways to give you a feel for the zeitgeist. I’ll discuss, firstly, the papers, then the workshops, and finally, and briefly, the keynotes.
Papers Here’s a ranked list of my top 8 papers. Most are on Offline RL, which is representative of the conference writ large.
1. Does Zero-Shot Reinforcement Learning Exist (Touati et. al, 2022)</description>
    </item>
    
    <item>
      <title>Low Emission Building Control with Zero-Shot Reinforcement Learning</title>
      <link>https://enjeeneer.io/projects/pearl/</link>
      <pubDate>Fri, 12 Aug 2022 09:34:36 +0100</pubDate>
      
      <guid>https://enjeeneer.io/projects/pearl/</guid>
      <description>In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 37, No. 12, pp. 14259-14267)
Scott R. Jeen\(^{1,3}\), Alessandro Abate\(^{2,3}\), Jonathan M. Cullen\(^{1}\) \(^{1}\) University of Cambridge
\(^{2}\) University of Oxford
\(^{3}\) Alan Turing Institute
[Paper] [Code] [Talk] [Poster]
   PEARL: Probabilistic Emission-Abating Reinforcement Learning  Presenting PEARL: Probabilistic Emission-Abating Reinforcement Learning, a deep RL algorithm that can find performant building control policies online, without pre-training&amp;ndash;the first time this has been shown to be possible.</description>
    </item>
    
    <item>
      <title>Zero Shot Building Control</title>
      <link>https://enjeeneer.io/talks/2022-06-23oxcav/</link>
      <pubDate>Thu, 23 Jun 2022 09:34:36 +0100</pubDate>
      
      <guid>https://enjeeneer.io/talks/2022-06-23oxcav/</guid>
      <description>Abstract
Heating and cooling systems in buildings account for 31% of global energy use, much of which are regulated by Rule Based Controllers (RBCs) that neither maximise energy efficiency nor minimise emissions by interacting optimally with the grid. Control via Reinforcement Learning (RL) has been shown to significantly improve building energy efficiency, but existing solutions require pre-training in simulators that are prohibitively expensive to obtain for every building in the world.</description>
    </item>
    
    <item>
      <title>One Hour RL</title>
      <link>https://enjeeneer.io/posts/2022/02/one-hour-rl/</link>
      <pubDate>Fri, 25 Feb 2022 15:23:20 +0000</pubDate>
      
      <guid>https://enjeeneer.io/posts/2022/02/one-hour-rl/</guid>
      <description>An Introduction to Reinforcement Learning Tom Bewley &amp;amp; Scott Jeen Alan Turing Institute 24/02/2022 The best way to walk through this tutorial is using the accompanying Jupyter Notebook:

[Jupyter Notebook]
1 | Markov Decision Processes: A Model of Sequential Decision Making 1.1. MDP (semi-)Formalism In reinforcement learning (RL), an agent takes actions in an environment to change its state over discrete timesteps $t$, with the goal of maximising the future sum of a scalar quantity known as reward.</description>
    </item>
    
    <item>
      <title>Presenting with Jupyter Notebooks</title>
      <link>https://enjeeneer.io/posts/2021/11/presenting-with-jupyter-notebooks/</link>
      <pubDate>Wed, 17 Nov 2021 09:03:11 -0500</pubDate>
      
      <guid>https://enjeeneer.io/posts/2021/11/presenting-with-jupyter-notebooks/</guid>
      <description>The best way to walk through this tutorial is using the accompanying Jupyter Notebook: 
[Jupyter Notebook]
- In the last year I&amp;rsquo;ve started presenting work using Jupyter Notebooks, rebelling against the Bill Gates&#39;-driven status-quo. Here I&amp;rsquo;ll explain how to do it. It&amp;rsquo;s not difficult, but in my opinion makes presentations look slicker, whilst allowing you to run code live in a presentation if you like. First, we need to download the plug-in that gives us the presentation functionality, it&amp;rsquo;s called RISE.</description>
    </item>
    
    <item>
      <title>Space Invasion, Space Invaders, and Climate Change Mitigation</title>
      <link>https://enjeeneer.io/talks/2021-06-03-jesusmcrgreen/</link>
      <pubDate>Thu, 03 Jun 2021 11:50:52 +0100</pubDate>
      
      <guid>https://enjeeneer.io/talks/2021-06-03-jesusmcrgreen/</guid>
      <description>Abstract
Recent advances in the field of Reinforcement Learning, a subfield of Artificial Intelligence, have shown computers can achieve superhuman performance at complex games like Go, Starcraft and the Atari Suite without human knowledge. What if, instead of playing Space Invaders, we play the game of climate change mitigation? More specifically, how can we design games where the goal is to minimise carbon emissions in some setting, and learn to play them optimally?</description>
    </item>
    
    <item>
      <title>Notes, Exercises and Code for Sutton and Barto&#39;s Reinforcement Learning: An Introduction (2018)</title>
      <link>https://enjeeneer.io/posts/2021/04/notes-exercises-and-code-for-sutton-and-bartos-reinforcement-learning-an-introduction-2018/</link>
      <pubDate>Fri, 30 Apr 2021 18:18:24 +0100</pubDate>
      
      <guid>https://enjeeneer.io/posts/2021/04/notes-exercises-and-code-for-sutton-and-bartos-reinforcement-learning-an-introduction-2018/</guid>
      <description>In the last few weeks I&amp;rsquo;ve been compiling a set of notes and exercise solutions for Sutton and Barto&amp;rsquo;s Reinforcement Learning: An Introduction. Admittedly, these were produced for my own benefit, but if you&amp;rsquo;d like to look at my notes, my (probably incorrect) answers to the exercises, or the code accommodating those answers, I&amp;rsquo;ll link directly to them below:
 Notes Exercises Code  Thanks to Bryn Hayder for inspiring this idea, and for providing his exercise solutions which helped me throughout.</description>
    </item>
    
    <item>
      <title>Scott&#39;s Uncomprehensive Guide to Scotland </title>
      <link>https://enjeeneer.io/posts/2021/04/scotts-uncomprehensive-guide-to-scotland/</link>
      <pubDate>Mon, 19 Apr 2021 21:08:05 +0000</pubDate>
      
      <guid>https://enjeeneer.io/posts/2021/04/scotts-uncomprehensive-guide-to-scotland/</guid>
      <description>Hello treasured friend. If you&amp;rsquo;re reading this, it&amp;rsquo;s probably because I&amp;rsquo;ve force-fed you a link after discussing your upcoming trip to Scotland. I hope this is useful to you in some way. Think of this a travel guide that you can dip into when you find yourself in one of these places either hungry or bored. I don&amp;rsquo;t describe anything in detail, you&amp;rsquo;ll just have to take me on my word that these places are worth visiting.</description>
    </item>
    
    <item>
      <title>Taking a peek inside black-box function approximators</title>
      <link>https://enjeeneer.io/talks/2021-03-15-xchangeseminar/</link>
      <pubDate>Mon, 15 Mar 2021 16:55:47 +0000</pubDate>
      
      <guid>https://enjeeneer.io/talks/2021-03-15-xchangeseminar/</guid>
      <description>Abstract
In this talk, I will introduce deterministic and probabilistic black-box function approximators, with applications for timeseries modelling. First, I introduce the multilayer perceptron (neural network), demonstrating how they can model arbitrarily complex functions, but highlighting their propensity to overfit the training data. Second, I introduce the Gaussian process (GP), a bayesian, probabilistic approach to function approximation. I visualise how GP&amp;rsquo;s quantify uncertainty, a robust trait in the context of control, but highlight their inability to scale to large datasets.</description>
    </item>
    
  </channel>
</rss>
