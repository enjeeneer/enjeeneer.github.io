<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Scott Jeen</title>
    <link>https://enjeeneer.io/</link>
    <description>Recent content on Scott Jeen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Fri, 14 Jun 2024 09:34:36 +0100</lastBuildDate><atom:link href="https://enjeeneer.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>The problem problem: choosing impactful research problems</title>
      <link>https://enjeeneer.io/talks/2024-06-14-reffciency/</link>
      <pubDate>Fri, 14 Jun 2024 09:34:36 +0100</pubDate>
      
      <guid>https://enjeeneer.io/talks/2024-06-14-reffciency/</guid>
      <description>[Click here for full-screen slideshow]
What are problems?  “A problem is a situation in which we experience conflicting ideas.”
David Deutsch
 Let’s start with a definition, here’s the best I’ve found for what constitutes a problem. David Deutsch says a problem is a situation in which we experience conflicting ideas. In other words we have two or more explanations about how we might proceed, and it&amp;rsquo;s not initially clear how to choose between them.</description>
    </item>
    
    <item>
      <title>Zero-Shot Reinforcement Learning from Low Quality Data</title>
      <link>https://enjeeneer.io/projects/zero-shot-rl/</link>
      <pubDate>Tue, 26 Sep 2023 17:27:21 +0100</pubDate>
      
      <guid>https://enjeeneer.io/projects/zero-shot-rl/</guid>
      <description>Scott Jeen\(^{1}\), Tom Bewley\(^{2}\), &amp;amp; Jonathan M. Cullen\(^{1}\) \(^{1}\) University of Cambridge
\(^{2}\) University of Bristol
[Paper] [Code] [Poster]
 Summary  Figure 1: Conservative zero-shot RL methods.
Zero-shot reinforcement learning (RL) concerns itself with learning general policies that can solve any unseen task in an environment. Recently, methods leveraging successor features and successor measures have emerged as viable zero-shot RL candidates, returning near-optimal policies for many unseen tasks. However, to enable this, they have assumed access to unrealistically large and heterogeneous datasets of transitions for pre-training.</description>
    </item>
    
    <item>
      <title>NeurIPS 2022</title>
      <link>https://enjeeneer.io/posts/2023/01/neurips-2022/</link>
      <pubDate>Thu, 26 Jan 2023 21:08:05 +0000</pubDate>
      
      <guid>https://enjeeneer.io/posts/2023/01/neurips-2022/</guid>
      <description>I was fortunate to attend NeurIPS in New Orleans in November. Here, I publish my takeaways to give you a feel for the zeitgeist. I’ll discuss, firstly, the papers, then the workshops, and finally, and briefly, the keynotes.
Papers Here’s a ranked list of my top 8 papers. Most are on Offline RL, which is representative of the conference writ large.
1. Does Zero-Shot Reinforcement Learning Exist (Touati et. al, 2022)</description>
    </item>
    
    <item>
      <title>Low Emission Building Control with Zero-Shot Reinforcement Learning</title>
      <link>https://enjeeneer.io/projects/pearl/</link>
      <pubDate>Fri, 12 Aug 2022 09:34:36 +0100</pubDate>
      
      <guid>https://enjeeneer.io/projects/pearl/</guid>
      <description>In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 37, No. 12, pp. 14259-14267)
Scott R. Jeen\(^{1,3}\), Alessandro Abate\(^{2,3}\), Jonathan M. Cullen\(^{1}\) \(^{1}\) University of Cambridge
\(^{2}\) University of Oxford
\(^{3}\) Alan Turing Institute
[Paper] [Code] [Talk] [Poster]
   PEARL: Probabilistic Emission-Abating Reinforcement Learning  Presenting PEARL: Probabilistic Emission-Abating Reinforcement Learning, a deep RL algorithm that can find performant building control policies online, without pre-training&amp;ndash;the first time this has been shown to be possible.</description>
    </item>
    
    <item>
      <title>Zero Shot Building Control</title>
      <link>https://enjeeneer.io/talks/2022-06-23oxcav/</link>
      <pubDate>Thu, 23 Jun 2022 09:34:36 +0100</pubDate>
      
      <guid>https://enjeeneer.io/talks/2022-06-23oxcav/</guid>
      <description>Abstract
Heating and cooling systems in buildings account for 31% of global energy use, much of which are regulated by Rule Based Controllers (RBCs) that neither maximise energy efficiency nor minimise emissions by interacting optimally with the grid. Control via Reinforcement Learning (RL) has been shown to significantly improve building energy efficiency, but existing solutions require pre-training in simulators that are prohibitively expensive to obtain for every building in the world.</description>
    </item>
    
    <item>
      <title>One Hour RL</title>
      <link>https://enjeeneer.io/posts/2022/02/one-hour-rl/</link>
      <pubDate>Fri, 25 Feb 2022 15:23:20 +0000</pubDate>
      
      <guid>https://enjeeneer.io/posts/2022/02/one-hour-rl/</guid>
      <description>An Introduction to Reinforcement Learning Tom Bewley &amp;amp; Scott Jeen Alan Turing Institute 24/02/2022 The best way to walk through this tutorial is using the accompanying Jupyter Notebook:

[Jupyter Notebook]
1 | Markov Decision Processes: A Model of Sequential Decision Making 1.1. MDP (semi-)Formalism In reinforcement learning (RL), an agent takes actions in an environment to change its state over discrete timesteps $t$, with the goal of maximising the future sum of a scalar quantity known as reward.</description>
    </item>
    
    <item>
      <title>Presenting with Jupyter Notebooks</title>
      <link>https://enjeeneer.io/posts/2021/11/presenting-with-jupyter-notebooks/</link>
      <pubDate>Wed, 17 Nov 2021 09:03:11 -0500</pubDate>
      
      <guid>https://enjeeneer.io/posts/2021/11/presenting-with-jupyter-notebooks/</guid>
      <description>The best way to walk through this tutorial is using the accompanying Jupyter Notebook: 
[Jupyter Notebook]
- In the last year I&amp;rsquo;ve started presenting work using Jupyter Notebooks, rebelling against the Bill Gates&#39;-driven status-quo. Here I&amp;rsquo;ll explain how to do it. It&amp;rsquo;s not difficult, but in my opinion makes presentations look slicker, whilst allowing you to run code live in a presentation if you like. First, we need to download the plug-in that gives us the presentation functionality, it&amp;rsquo;s called RISE.</description>
    </item>
    
    <item>
      <title>Space Invasion, Space Invaders, and Climate Change Mitigation</title>
      <link>https://enjeeneer.io/talks/2021-06-03-jesusmcrgreen/</link>
      <pubDate>Thu, 03 Jun 2021 11:50:52 +0100</pubDate>
      
      <guid>https://enjeeneer.io/talks/2021-06-03-jesusmcrgreen/</guid>
      <description>Abstract
Recent advances in the field of Reinforcement Learning, a subfield of Artificial Intelligence, have shown computers can achieve superhuman performance at complex games like Go, Starcraft and the Atari Suite without human knowledge. What if, instead of playing Space Invaders, we play the game of climate change mitigation? More specifically, how can we design games where the goal is to minimise carbon emissions in some setting, and learn to play them optimally?</description>
    </item>
    
    <item>
      <title>Notes, Exercises and Code for Sutton and Barto&#39;s Reinforcement Learning: An Introduction (2018)</title>
      <link>https://enjeeneer.io/posts/2021/04/notes-exercises-and-code-for-sutton-and-bartos-reinforcement-learning-an-introduction-2018/</link>
      <pubDate>Fri, 30 Apr 2021 18:18:24 +0100</pubDate>
      
      <guid>https://enjeeneer.io/posts/2021/04/notes-exercises-and-code-for-sutton-and-bartos-reinforcement-learning-an-introduction-2018/</guid>
      <description>In the last few weeks I&amp;rsquo;ve been compiling a set of notes and exercise solutions for Sutton and Barto&amp;rsquo;s Reinforcement Learning: An Introduction. Admittedly, these were produced for my own benefit, but if you&amp;rsquo;d like to look at my notes, my (probably incorrect) answers to the exercises, or the code accommodating those answers, I&amp;rsquo;ll link directly to them below:
 Notes Exercises Code  Thanks to Bryn Hayder for inspiring this idea, and for providing his exercise solutions which helped me throughout.</description>
    </item>
    
    <item>
      <title>Scott&#39;s Uncomprehensive Guide to Scotland </title>
      <link>https://enjeeneer.io/posts/2021/04/scotts-uncomprehensive-guide-to-scotland/</link>
      <pubDate>Mon, 19 Apr 2021 21:08:05 +0000</pubDate>
      
      <guid>https://enjeeneer.io/posts/2021/04/scotts-uncomprehensive-guide-to-scotland/</guid>
      <description>Hello treasured friend. If you&amp;rsquo;re reading this, it&amp;rsquo;s probably because I&amp;rsquo;ve force-fed you a link after discussing your upcoming trip to Scotland. I hope this is useful to you in some way. Think of this a travel guide that you can dip into when you find yourself in one of these places either hungry or bored. I don&amp;rsquo;t describe anything in detail, you&amp;rsquo;ll just have to take me on my word that these places are worth visiting.</description>
    </item>
    
  </channel>
</rss>
