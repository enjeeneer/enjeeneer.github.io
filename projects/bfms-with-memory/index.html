<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="Scott Jeen">
<meta name="description" content="RL Conference 2025 Scott Jeen\(^{1}\), Tom Bewley, &amp;amp; Jonathan M. Cullen\(^{1}\) \(^{1}\) University of Cambridge
[Paper] [Code]
  Figure 1: BFMs with memory. In the case of FB, the forward model F and backward model B condition on the output of memory models that compress trajectories of observations and actions to estimate the underlying state. 
Summary Behaviour foundation models (BFMs) can learn general policies that solve any unseen task in an environment under certain assumptions." />
<meta name="keywords" content="Scott Jeen" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="#6B8E23" />
<link rel="canonical" href="https://enjeeneer.io/projects/bfms-with-memory/" />


<script async src="https://www.googletagmanager.com/gtag/js?id=G-P95NFMWGS8"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-P95NFMWGS8', { 'anonymize_ip': false });
}
</script>



    <title>
        
            Zero-Shot Reinforcement Learning Under Partial Observability :: Scott Jeen  — AI researcher
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="/main.cbec4bded5981a9639be0a9253dbc5636fb308fba08c31a50670aee0a034a132.css">




    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#6B8E23">
    <link rel="shortcut icon" href="/favicon.ico">
    <meta name="msapplication-TileColor" content="#6B8E23">
    <meta name="theme-color" content="#6B8E23">



<meta itemprop="name" content="Zero-Shot Reinforcement Learning Under Partial Observability">
<meta itemprop="description" content="RL Conference 2025 Scott Jeen\(^{1}\), Tom Bewley, &amp; Jonathan M. Cullen\(^{1}\) \(^{1}\) University of Cambridge
[Paper] [Code]
  Figure 1: BFMs with memory. In the case of FB, the forward model F and backward model B condition on the output of memory models that compress trajectories of observations and actions to estimate the underlying state. 
Summary Behaviour foundation models (BFMs) can learn general policies that solve any unseen task in an environment under certain assumptions."><meta itemprop="datePublished" content="2025-06-12T21:48:13+01:00" />
<meta itemprop="dateModified" content="2025-06-12T21:48:13+01:00" />
<meta itemprop="wordCount" content="677"><meta itemprop="image" content="https://enjeeneer.io"/>
<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://enjeeneer.io"/>

<meta name="twitter:title" content="Zero-Shot Reinforcement Learning Under Partial Observability"/>
<meta name="twitter:description" content="RL Conference 2025 Scott Jeen\(^{1}\), Tom Bewley, &amp; Jonathan M. Cullen\(^{1}\) \(^{1}\) University of Cambridge
[Paper] [Code]
  Figure 1: BFMs with memory. In the case of FB, the forward model F and backward model B condition on the output of memory models that compress trajectories of observations and actions to estimate the underlying state. 
Summary Behaviour foundation models (BFMs) can learn general policies that solve any unseen task in an environment under certain assumptions."/>



    <meta property="og:title" content="Zero-Shot Reinforcement Learning Under Partial Observability" />
<meta property="og:description" content="RL Conference 2025 Scott Jeen\(^{1}\), Tom Bewley, &amp; Jonathan M. Cullen\(^{1}\) \(^{1}\) University of Cambridge
[Paper] [Code]
  Figure 1: BFMs with memory. In the case of FB, the forward model F and backward model B condition on the output of memory models that compress trajectories of observations and actions to estimate the underlying state. 
Summary Behaviour foundation models (BFMs) can learn general policies that solve any unseen task in an environment under certain assumptions." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://enjeeneer.io/projects/bfms-with-memory/" /><meta property="og:image" content="https://enjeeneer.io"/><meta property="article:section" content="projects" />
<meta property="article:published_time" content="2025-06-12T21:48:13+01:00" />
<meta property="article:modified_time" content="2025-06-12T21:48:13+01:00" />







    <meta property="article:published_time" content="2025-06-12 21:48:13 &#43;0100 BST" />








<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script>

    
    </head>

    <body class="">
        <div class="container">
            <header class="header">
    
<script async src="https://www.googletagmanager.com/gtag/js?id=G-P95NFMWGS8"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-P95NFMWGS8', { 'anonymize_ip': false });
}
</script>

    <span class="header__inner">
        <a href="/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text">/enjeeneer/</span>
            <span class="logo__cursor" style=
                  "
                   background-color:#6B8E23;
                   animation-duration:1s;">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://enjeeneer.io/about/">about</a></li><li><a href="https://enjeeneer.io/posts/">posts</a></li><li><a href="https://enjeeneer.io/projects/">projects</a></li><li><a href="https://scholar.google.com/citations?user=3HPX720AAAAJ&amp;hl=en&amp;oi=ao">scholar</a></li><li><a href="https://enjeeneer.io/talks/">talks</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            

            <span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>
</span>
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            
            </p>
        </div>

        <article>
            <h2 class="post-title"><a href="https://enjeeneer.io/projects/bfms-with-memory/">Zero-Shot Reinforcement Learning Under Partial Observability</a></h2>
            <h3 class="loc"><a href="https://enjeeneer.io/projects/bfms-with-memory/"></a></h3>
            <h3 class="time"><a href="https://enjeeneer.io/projects/bfms-with-memory/"></a></h3>

            

            

            <div class="post-content">
                <h2 id="rl-conference-2025">RL Conference 2025</h2>
<h3 id="scott-jeen1-tom-bewley--jonathan-m-cullen1">Scott Jeen\(^{1}\), Tom Bewley, &amp; Jonathan M. Cullen\(^{1}\)</h3>
<p>\(^{1}\) <em>University of Cambridge</em></p>
<p><strong><a href="https://arxiv.org/abs/2506.15446">[Paper]</a></strong>
<strong><a href="https://github.com/enjeeneer/bfms-with-memory">[Code]</a></strong></p>
<hr>
<p><figure><img src="https://github.com/enjeeneer/bfms-with-memory/blob/main/assets/fb-with-memory-architecture.png?raw=true"/>
</figure>

<small><em>Figure 1: <strong>BFMs with memory</strong>.  In the case of FB, the forward model F and backward model B condition on
the output of memory models that compress trajectories of observations and actions to estimate the underlying state.</em> </small></p>
<h2 id="summary">Summary</h2>
<p>Behaviour foundation models (BFMs) can learn general policies that solve <em>any</em> unseen task in an environment under certain assumptions.
BFMs have so far assumed access to Markov states that provide all the information required to solve a task,
but in most real problems this is an unrealistic assumption and the state is only <em>partially observed</em>. The general fix for partial observability is to
use <em>memory models</em> (like Transformers and RNNs) to estimate underlying states from histories of observations and actions. This work
augments BFMs with memory models to improve their performance when subjected to partial observability.</p>
<hr>
<h2 id="intuition">Intuition</h2>
<p>When BFMs are fed noisy or partial observations instead of clean states, they break in predictable ways (Figure 2). The forward model,
which predicts how the world will evolve, misrepresents the true dynamics—this throws off the Q-values and leads to bad
action choices. Meanwhile, the backward model, which tries to infer what task the agent is solving based
on where it ends up, also fails because it can’t reliably associate partial observations with the correct reward structure.
These breakdowns—state and task misidentification—undermine the policy’s ability to plan effectively. Our fix? Plug
in memory models to help the BFM reconstruct the underlying state and task from past observations and actions.
This lets us keep the same architecture, but extend its powers to messy, partially observed settings.</p>
<figure><img src="https://github.com/enjeeneer/bfms-with-memory/blob/main/assets/fb-pomdp-failure-mode.png?raw=true"/>
</figure>

<p><small><em>Figure 2: <strong>The failure modes of BFMs under partial observability</strong>. FB’s average (IQM) all-task return
on Walker when observations are passed to its respective components. Observations are created by adding
Gaussian noise to the underlying states. (Left) Observations are passed as input to B causing FB to misidentify
the task. (Middle) Observations are passed as input to F and π causing FB to misidentify the state. (Right)
Observations are passed as input to F, π and B causing FB to misidentify both the task and state.</em> </small></p>
<hr>
<h2 id="results">Results</h2>
<p>We demonstrate our methods improve performance w.r.t memory free BFMs on ExORL environments adapted to exhibit
partially observed states (Figure 3) and changes in dynamics (Figure 4).
<figure><img src="https://github.com/enjeeneer/bfms-with-memory/blob/main/assets/results-generalisation.png?raw=true" width="100%"/>
</figure>
</p>
<p><small>_Figure 3: <strong>Aggregate zero-shot task performance on ExORL with partially observed states.</strong> IQM of task
scores across all tasks on noisy and flickering variants of Walker, Cheetah and Quadruped, normalised
against the performance of FB in the fully observed environment. 5 random seeds.</small></p>
<figure><img src="https://github.com/enjeeneer/bfms-with-memory/blob/main/assets/results-states.png?raw=true" width="100%"/>
</figure>

<p><small><em>Figure 4: <strong>Aggregate zero-shot task performance on ExORL with changed dynamics at test time.</strong> IQM of
task scores across all tasks when trained on dynamics where mass and damping coefficients are scaled to {0.5×,
1.5×} their usual values and evaled on {1.0×, 2.0×} their usual values, normalised against the performance of
FB in the fully observed environment. To solve the test dynamics with 1.0× scaling the agent must interpolate
within the training set; to solve the test dynamics with 2.0× scaling the agent must extrapolate from the training
set.</em></small></p>
<p>Interestingly, we find that GRUs are the most performant memory model for BFMs, outperforming Transformers and S4d. In particular,
we find it is the only memory model that can be trained stably when applied to both forward and backward models.
<figure><img src="https://github.com/enjeeneer/bfms-with-memory/blob/main/assets/results-memory-models.png?raw=true" width="100%"/>
</figure>
</p>
<p><small><em>Figure 5: <strong>Aggregate zero-shot task performance of FB-M with different memory models.</strong> IQM of task
scores across all tasks on Walker flickering. (Left) Observations are passed only to a memory-based backward model; the forward model and policy are memory-free. (Middle) Observations are passed only to the
forward model and policy; the backward model is memory-free. (Right) Observations are passed to all models.</em></small></p>
<hr>
<p>Check out the <a href="https://arxiv.org/abs/2506.15446">full paper</a> for more details! If you find this work informative, please
consider citing the paper:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">@article<span style="color:#f92672">{</span>jeen2025,
  url <span style="color:#f92672">=</span> <span style="color:#f92672">{</span>https://arxiv.org/abs/2309.15178<span style="color:#f92672">}</span>,
  author <span style="color:#f92672">=</span> <span style="color:#f92672">{</span>Jeen, Scott and Bewley, Tom and Cullen, Jonathan M.<span style="color:#f92672">}</span>,  
  title <span style="color:#f92672">=</span> <span style="color:#f92672">{</span>Zero-Shot Reinforcement Learning Under Partial Observability<span style="color:#f92672">}</span>,
  journal <span style="color:#f92672">=</span> <span style="color:#f92672">{</span>2nd Reinforcement Learning Conference<span style="color:#f92672">}</span>,
  year <span style="color:#f92672">=</span> <span style="color:#f92672">{</span>2025<span style="color:#f92672">}</span>,
<span style="color:#f92672">}</span>
</code></pre></div><hr>

            </div>
        </article>

        <hr />

        <div class="post-info">
            
            
  		</div>

    </main>

            </div>

            
        </div>

        




<script type="text/javascript" src="/bundle.min.a0f363fdf81cdc5cfacc447a79c33189eb000d090336cd04aac8ee256f423b3133b836c281944c19c75e38d0b0b449f01ce5807e37798b7ac94ac1db51983fc4.js" integrity="sha512-oPNj/fgc3Fz6zER6ecMxiesADQkDNs0EqsjuJW9COzEzuDbCgZRMGcdeONCwtEnwHOWAfjd5i3rJSsHbUZg/xA=="></script>
    
<script async src="https://www.googletagmanager.com/gtag/js?id=G-P95NFMWGS8"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-P95NFMWGS8', { 'anonymize_ip': false });
}
</script>




    </body>
</html>
