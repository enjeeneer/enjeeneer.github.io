<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Scott Jeen</title>
    <link>https://enjeeneer.io/</link>
    <description>Recent content on Scott Jeen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Thu, 12 Jun 2025 21:48:13 +0100</lastBuildDate><atom:link href="https://enjeeneer.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Zero-Shot Reinforcement Learning Under Partial Observability</title>
      <link>https://enjeeneer.io/projects/bfms-with-memory/</link>
      <pubDate>Thu, 12 Jun 2025 21:48:13 +0100</pubDate>
      
      <guid>https://enjeeneer.io/projects/bfms-with-memory/</guid>
      <description>RL Conference 2025 Scott Jeen\(^{1}\), Tom Bewley, &amp;amp; Jonathan M. Cullen\(^{1}\) \(^{1}\) University of Cambridge
[Paper] [Code]
  Figure 1: BFMs with memory. In the case of FB, the forward model F and backward model B condition on the output of memory models that compress trajectories of observations and actions to estimate the underlying state. 
Summary Behaviour foundation models (BFMs) can learn general policies that solve any unseen task in an environment under certain assumptions.</description>
    </item>
    
    <item>
      <title>20s</title>
      <link>https://enjeeneer.io/posts/2025/03/20s/</link>
      <pubDate>Mon, 31 Mar 2025 21:15:18 +0100</pubDate>
      
      <guid>https://enjeeneer.io/posts/2025/03/20s/</guid>
      <description>I turned 30 today. Here are some particularly important moments from the last decade.
Highs
 drives to KB on frosty mornings with hamilton, hayden and adam lcd soundsystem on the west side highway with the roof down the beginning of infinity glasto nights with dom, deirdre and cat nablus with muath, fin, calum, dom and hayden francis and simone’s wedding david silver’s lectures and the first reading of sutton and barto sunny evening walks with cat around cambridge submitting the phd lunch conversations at fdm late night snacks in brooklyn after a double date sherkin island with cat ollie worrall on the 16th green at aldeburgh ashby lab with timo and josh the first drive down the backs past king’s college  Lows</description>
    </item>
    
    <item>
      <title>PhD Viva</title>
      <link>https://enjeeneer.io/talks/viva/</link>
      <pubDate>Wed, 19 Feb 2025 22:20:33 +0000</pubDate>
      
      <guid>https://enjeeneer.io/talks/viva/</guid>
      <description>[Click here for full-screen slideshow]</description>
    </item>
    
    <item>
      <title>Zero-Shot Reinforcement Learning from Low Quality Data</title>
      <link>https://enjeeneer.io/talks/neurips/</link>
      <pubDate>Tue, 10 Dec 2024 17:01:15 -0400</pubDate>
      
      <guid>https://enjeeneer.io/talks/neurips/</guid>
      <description>[Click here for full-screen slideshow]</description>
    </item>
    
    <item>
      <title>4 Years in 4(ish) Minutes</title>
      <link>https://enjeeneer.io/talks/2024-11-12-phd-reflection/</link>
      <pubDate>Tue, 12 Nov 2024 14:18:07 +0000</pubDate>
      
      <guid>https://enjeeneer.io/talks/2024-11-12-phd-reflection/</guid>
      <description>[Click here for full-screen slideshow]</description>
    </item>
    
    <item>
      <title>Searching for useful problems</title>
      <link>https://enjeeneer.io/posts/2024/08/searching-for-useful-problems/</link>
      <pubDate>Mon, 26 Aug 2024 19:36:27 +0200</pubDate>
      
      <guid>https://enjeeneer.io/posts/2024/08/searching-for-useful-problems/</guid>
      <description>This post is based on a talk I gave to my research group at Cambridge University in June 2024. You can find the notes and slides for the talk here.
Solutions to most problems aren’t particularly useful. Solutions to a small number of problems are extremely useful. If you’re interested in doing good, you’ll want to search for problems that look like the latter. It’s a hard search, but the ROI is likely greater than any other use of your time, and I have some ways of running it that I think make it a little more tractable.</description>
    </item>
    
    <item>
      <title>Dynamics Generalisation with Behaviour Foundation Models</title>
      <link>https://enjeeneer.io/talks/2024-08-09-rlc-tafm/</link>
      <pubDate>Wed, 07 Aug 2024 17:01:15 -0400</pubDate>
      
      <guid>https://enjeeneer.io/talks/2024-08-09-rlc-tafm/</guid>
      <description>[Click here for full-screen slideshow]</description>
    </item>
    
    <item>
      <title>The problem problem: choosing impactful research problems</title>
      <link>https://enjeeneer.io/talks/2024-06-14-reffciency/</link>
      <pubDate>Fri, 14 Jun 2024 09:34:36 +0100</pubDate>
      
      <guid>https://enjeeneer.io/talks/2024-06-14-reffciency/</guid>
      <description>[Click here for full-screen slideshow]
What are problems?  “A problem is a situation in which we experience conflicting ideas.”
David Deutsch
 Let’s start with a definition, here’s the best I’ve found for what constitutes a problem. David Deutsch says a problem is a situation in which we experience conflicting ideas. In other words we have two or more explanations about how we might proceed, and it&amp;rsquo;s not initially clear how to choose between them.</description>
    </item>
    
    <item>
      <title>Zero-Shot Reinforcement Learning from Low Quality Data</title>
      <link>https://enjeeneer.io/projects/zero-shot-rl/</link>
      <pubDate>Tue, 26 Sep 2023 17:27:21 +0100</pubDate>
      
      <guid>https://enjeeneer.io/projects/zero-shot-rl/</guid>
      <description>NeurIPS 2024 Scott Jeen\(^{1}\), Tom Bewley\(^{2}\), &amp;amp; Jonathan M. Cullen\(^{1}\) \(^{1}\) University of Cambridge
\(^{2}\) University of Bristol
[Paper] [Code] [Poster] [Slides]
 Summary Zero-shot reinforcement learning (RL) methods learn general policies that can, in principle, solve any unseen task in an environment. Recently, methods leveraging successor features and successor measures have emerged as viable zero-shot RL candidates, returning near-optimal policies for many unseen tasks. However, to enable this, they have assumed access to unrealistically large and heterogeneous datasets of transitions for pre-training.</description>
    </item>
    
    <item>
      <title>NeurIPS 2022</title>
      <link>https://enjeeneer.io/posts/2023/01/neurips-2022/</link>
      <pubDate>Thu, 26 Jan 2023 21:08:05 +0000</pubDate>
      
      <guid>https://enjeeneer.io/posts/2023/01/neurips-2022/</guid>
      <description>I was fortunate to attend NeurIPS in New Orleans in November. Here, I publish my takeaways to give you a feel for the zeitgeist. I’ll discuss, firstly, the papers, then the workshops, and finally, and briefly, the keynotes.
Papers Here’s a ranked list of my top 8 papers. Most are on Offline RL, which is representative of the conference writ large.
1. Does Zero-Shot Reinforcement Learning Exist (Touati et. al, 2022)</description>
    </item>
    
  </channel>
</rss>
