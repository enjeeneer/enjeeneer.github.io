<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Scott Jeen</title>
        <link>https://enjeeneer.io/posts/</link>
        <description>Recent content in Posts on Scott Jeen</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
        <lastBuildDate>Thu, 26 Jan 2023 21:08:05 +0000</lastBuildDate>
        <atom:link href="https://enjeeneer.io/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>NeurIPS 2022</title>
            <link>https://enjeeneer.io/posts/2023/01/neurips-2022/</link>
            <pubDate>Thu, 26 Jan 2023 21:08:05 +0000</pubDate>
            
            <guid>https://enjeeneer.io/posts/2023/01/neurips-2022/</guid>
            <description>I was fortunate to attend NeurIPS in New Orleans in November. Here, I publish my takeaways to give you a feel for the zeitgeist. I’ll discuss, firstly, the papers, then the workshops, and finally, and briefly, the keynotes.
Papers Here’s a ranked list of my top 8 papers. Most are on Offline RL, which is representative of the conference writ large, but one relates to the (imo) under discussed subfield of zero-shot RL.</description>
            <content type="html"><![CDATA[<p>I was fortunate to attend NeurIPS in New Orleans in November. Here, I publish my takeaways to give you a feel for the zeitgeist. I’ll discuss, firstly, the papers, then the workshops, and finally, and briefly, the keynotes.</p>
<h2 id="papers">Papers</h2>
<p>Here’s a ranked list of my top 8 papers. Most are on Offline RL, which is representative of the conference writ large, but one relates to the (imo) under discussed subfield of zero-shot RL.</p>
<p><strong>1. <a href="https://arxiv.org/pdf/2209.14935.pdf">Does Zero-Shot Reinforcement Learning Exist (Touati et. al, 2022)</a></strong></p>
<figure><img src="/img/toutati2022.jpg"/>
</figure>

<p><strong>Key idea.</strong> To do zero-shot RL, we need to learn a general function from reward-free transitions that implicitly encodes the trajectories of <strong>all</strong> optimal policies for <strong>all</strong> tasks. The authors propose to learn two functions: \(F_\theta(s)\) and  \(B_\phi(s)\) that encode the future and past of state \(s\). We want to learn functions that <strong>always</strong> find a route from \(s \rightarrow s'\).</p>
<p><strong>Implication(s):</strong></p>
<ul>
<li>They beat every previous zero-shot RL algorithm on the standard offline RL tasks, and approach the performance of online, reward-guided RL algorithms in some envs.</li>
</ul>
<p><strong>Misc thoughts:</strong></p>
<ul>
<li>It seems clear that zero-shot RL is the route to real world deployment for RL. This work represents the best effort I’ve seen in this direction. I’m really excited by it and will be looking to extend it in my own future work.</li>
</ul>
<hr>
<p><strong>2. <a href="https://arxiv.org/pdf/2206.05314.pdf">Large Scale Retrieval for Reinforcement Learning (Humphreys et. al, 2022)</a></strong></p>
<figure><img src="/img/largescaleretrieval.png" width="500" height="600"/>
</figure>

<p><strong>Key idea.</strong> Assuming access to a large offline dataset, we perform a nearest neighbours search over the dataset w.r.t. the current state, and append the retrieved states, next actions, rewards and final states (in the case of go) to the current state. The policy then acts w.r.t this augmented state.</p>
<p>Implication(s):</p>
<ul>
<li><strong>Halves</strong> compute required to achieve the baseline win-rate in Go.</li>
</ul>
<p><strong>Misc thoughts:</strong></p>
<ul>
<li>This represents the most novel approach to offline RL I’ve seen; most techniques separate the offline and online learning phases, but here the authors combine them elegantly.</li>
<li>To me this feels like a far more promising approach to offline RL than CQL etc.</li>
</ul>
<hr>
<p><strong>3. <a href="https://arxiv.org/pdf/2206.00730.pdf">The Phenomenon of Policy Churn (Schaul et. al, 2022)</a></strong></p>
<p><figure><img src="/img/churn1.png"/>
</figure>

<figure><img src="/img/churn2.png"/>
</figure>
</p>
<p><strong>Key idea.</strong> When a value-based agent acts greedily, the policy updates by a surprising amount per gradient step e.g. in up to 10% of states in some cases.</p>
<p><strong>Implication(s):</strong></p>
<ul>
<li>Policy churn means that ((\epsilon))-greedy exploration may not be required as a rapidly changing policy induces enough noise into the data distribution that exploration may be implicit.</li>
</ul>
<p><strong>Misc thoughts:</strong></p>
<ul>
<li>Their paper is structured in a really engaging way.</li>
<li>I liked their ML researcher survey which quantified how surprising their result was to experts.</li>
</ul>
<hr>
<p><strong>4. <a href="https://arxiv.org/pdf/2206.08853.pdf">MINEDOJO: Building Open-Ended Embodied Agents with Internet-Scale Knowledge (Fan et. al, 2022)</a></strong></p>
<figure><img src="/img/minedojo.jpg"/>
</figure>

<p><strong>Key idea.</strong> An internet-scale benchmark for generalist RL agents. 1000s of tasks, and a limitless procedurally-generated world for training.</p>
<p><strong>Implication(s):</strong></p>
<ul>
<li>Provides a sufficiently diverse and complex sandbox for training more generally capable agents.</li>
</ul>
<p><strong>Misc thoughts:</strong></p>
<ul>
<li>This is an amazing software development effort from a relatively small team. Jim Fan is so cool!</li>
</ul>
<hr>
<p><strong>5. <a href="https://arxiv.org/pdf/2210.05805.pdf">Exploration via Elliptical Episodic Bonuses (Henaff et. al, 2022)</a></strong></p>
<figure><img src="/img/ellipticalbonus.png"/>
</figure>

<p><strong>Key Idea.</strong> Guided exploration is often performed by providing the agent reward inversely proportional to the state visitation count i.e. if you haven’t visited this state much you receive added reward. This works for discrete state spaces, but in continuous state spaces each visited state is ~ unique. Here, the authors parameterise ellipses around visited states, specifying a <em>region</em> of nearby states, outside of which the agent receives added reward.</p>
<p><strong>Implication(s):</strong></p>
<ul>
<li>Better exploration means SOTA on the mini-hack suite of envs</li>
<li>Strong performance of reward-free exploration tasks i.e. this is a really good way of thinking about exploration.</li>
</ul>
<p><strong>Misc. thoughts:</strong></p>
<ul>
<li>I really liked the elegance of this idea. A good example of simple, well-examined ideas being useful to the community.</li>
</ul>
<hr>
<p><strong>6. <a href="https://arxiv.org/pdf/2205.15967.pdf">You Can’t Count on Luck: Why Decision Transformers and RvS Fail in Stochastic Environments (Paster et al., 2022)</a></strong></p>
<figure><img src="/img/luck.png"/>
</figure>

<p><strong>Key Idea.</strong> In a stochastic environment, trajectories in a dataset used to train decision transformer may be high-reward by chance. Here the authors cluster similar trajectories and find their expected reward to mitigate overfitting to lucky trajectories.</p>
<p><strong>Implication(s):</strong></p>
<ul>
<li>Decision transformer trained on these new objectives exhibits policies that area better aligned with the return conditioning of the user.</li>
</ul>
<p><strong>Misc. thoughts:</strong></p>
<ul>
<li>Another simple idea with positive implications for performance.</li>
</ul>
<hr>
<p><strong>7. <a href="https://nips.cc/virtual/2022/poster/52843">Multi-Game Decision Transformers</a> (Lee et al., 2022)</strong></p>
<figure><img src="/img/multigame.png"/>
</figure>

<p><strong>Key idea.</strong> Instead of predicting just the next action conditioned on state and return-to-go like the original decision transformer paper, they predict the intermediate reward and return-to-go. This allows them to re-condition on new returns-to-go at each timestep, using a clever sampling procedure that samples likely expert returns-to-go.</p>
<p><strong>Implication(s):</strong></p>
<ul>
<li>SOTA on standard atari offline RL tasks.</li>
</ul>
<p><strong>Misc thoughts:</strong></p>
<ul>
<li>This work is very similar to the original decision transformer paper, so I’m surprised that it received a best paper award.</li>
<li>It represents continued progress in the field on offline RL, and more specifically, decision transformer style architectures.</li>
</ul>
<hr>
<p><strong>8. <a href="https://arxiv.org/abs/2206.01079">When does return-conditioned supervised learning work for offline reinforcement learning? (Brandfonbrener, 2022)</a></strong></p>
<p><strong>Key idea.</strong> Much recent work on offline RL can be cast as supervised learning on a near-optimal offline dataset then conditioning on high rewards from the dataset at test time; under what conditions is this a valid approach? Here the authors prove that this (unsurprisingly) only works when two conditions are met: 1) the test envs are (nearly) deterministic, and 2) there is trajectory-level converage in the dataset.</p>
<p><strong>Implication(s):</strong></p>
<ul>
<li>Current approaches to offline RL will not work in the real world because real envs are generally stochastic.</li>
</ul>
<p><strong>Misc thoughts:</strong></p>
<ul>
<li>I liked that the authors proved the community’s intuitions on current approaches to offline RL that, although somewhat obvious in retrospect, had not been verified.</li>
</ul>
<hr>
<h2 id="workshops">Workshops</h2>
<p>I attended 5 workshops:</p>
<ol>
<li>Foundation Models for Decision Making</li>
<li>Safety</li>
<li>Offline RL</li>
<li>Real Life Reinforcement Learning</li>
<li>Tackling Climate Change with Machine Learning</li>
</ol>
<p>I found the latter three to be interesting, but less informative and precient as the first two. I therefore only discuss the Foundation Models for Decision Making and Safety workshops; the extent to which I enjoyed both workshops is, in a sense, oxymoronic.</p>
<h3 id="foundation-models-for-decision-making">Foundation Models for Decision Making</h3>
<p><strong>Leslie P. Kaelbling: What does an intelligent robot need to know?</strong></p>
<figure><img src="/img/kaelbling2022.jpg"/>
</figure>

<p>My favourite talk was from <a href="https://scholar.google.com/citations?user=IcasIiwAAAAJ&amp;hl=en">Leslie Kaelbling</a> of MIT. Kaelbling focussed on our proclivity for building inductive biases into our models (a similar thesis to Sutton’s <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">Bitter Lesson</a>); though good in short term, the effectiveness of such priors plateaus in the long-run. I agree with her.</p>
<p>She advocates for a marketplace of pre-trained models of the following types:</p>
<ul>
<li>Foundation: space, geometry, kinematics</li>
<li>Psychology: other agents, beliefs, desires etc.</li>
<li>Culture: how do u do things in the world e.g. stuff you can read in books</li>
</ul>
<p>Robotics manufacturers will provide:</p>
<ul>
<li>observation / perception</li>
<li>actuators</li>
<li>controllers e.g. policies</li>
</ul>
<p>And we’ll use our own expertise to build local states (specific facts about the env) and encode long horizon memories e.g. what did I do 2 years ago.</p>
<hr>
<h3 id="safety-unofficial-in-the-marriott-across-the-road">Safety (unofficial; in the Marriott across the road)</h3>
<p>The safety workshop was wild. It was a small, unofficial congregation of researchers who you’d expect to see lurking on <a href="https://www.lesswrong.com/">Less Wrong</a> and other <a href="https://forum.effectivealtruism.org/">EA forums</a>.</p>
<p><strong><a href="http://christoph-schuhmann.de/">Christoph Schuhmann</a> (Founder of LAION)</strong></p>
<p>Chris is a high school teacher from Vienna; he gave an inspiring talk on the open-sourcing of foundation models. He started LAION (Large-scale Artificial Intelligence Open Network) a non-profit organization, provides datasets, tools and models to democratise ML research. His key points included:</p>
<ul>
<li>centralised intelligence means centralised problem solving; we can’t give the keys to problem solving to a (potentially) dictatorial few.</li>
<li>risks by not open sourcing AI are bigger than those of open sourcing</li>
<li>LAION progress:
<ul>
<li>initial plan was to replicate the orignal CLIP / Dalle-1</li>
<li>got 3m image text pairs on his own</li>
<li>discord server helped him get 300m image text pairs, then 5b pairs</li>
<li>hedge fund gave them 8 A100s</li>
</ul>
</li>
<li>We will always want to do things even if AI can, cause we need to express ourselve</li>
</ul>
<p><strong>Thomas Wolf (Hugging Face CEO)</strong></p>
<p>Tom Wolf gave a talk on the <a href="https://www.notion.so/NeurIPS-a25bdf6d9af045f9bff65177f2833cfa">Big Science initiative</a>, a project takes inspiration from scientific creation schemes such as CERN and the LHC, in which open scientific collaborations facilitate the creation of large-scale artefacts that are useful for the entire research community:</p>
<ul>
<li>1000+ researchers coming together to build massive language model and massive dataset</li>
<li>efficient agi will probs require modularity cc. LeCun</li>
<li>working on the energy efficiency of training is inherently democratic i.e. stops models being held by the rich, especially re: inference</li>
</ul>
<p>Are AI researchers aligned on AGI alignment?</p>
<p>There was interesting round table at the end of the workshop that included <a href="https://scholar.google.com/citations?user=KNr3vb4AAAAJ&amp;hl=en">Jared Kaplan</a> (Anthropic) and <a href="https://scholar.google.ca/citations?user=5Uz70IoAAAAJ&amp;hl=en">David Krueger</a> (Cambridge) discussing what is means to align AGI. There was little agreement.</p>
<hr>
<h2 id="keynotes">Keynotes</h2>
<p>I attended 4 of the 6 keynotes which were:</p>
<ol>
<li><strong>David Chalmers:</strong> <a href="https://nips.cc/virtual/2022/invited-talk/55867">Are Large Language Models Sentient?</a></li>
<li><strong>Emmanuel Candes:</strong> <a href="https://nips.cc/virtual/2022/invited-talk/55872">Conformal Prediction in 2022</a></li>
<li><strong>Isabelle Guyon:</strong> <a href="https://nips.cc/virtual/2022/invited-talk/56158">The Data-Centric Era: How ML is Becoming an Experimental Science</a></li>
<li><strong>Geoff Hinton:</strong> <a href="https://nips.cc/virtual/2022/invited-talk/55869">The Forward-Forward Algorithm for Training Deep Neural Networks</a></li>
</ol>
<p>I found Emmanuel’s talk on conformal prediction enlightening as I’d never heard of the topic (<a href="https://arxiv.org/abs/2107.07511#:~:text=Conformal%20prediction%20is%20a%20user,distributional%20assumptions%20or%20model%20assumptions.">here’s a primer</a>), and Isabelle’s talk on benchmark and data transparency to be agreeable, if a little unoriginal. Hinton’s talk on a more anatomically correct learning algorithm was interesting, but I’m as yet unconvinced that mimicking human intelligence is a good way of building systems that are superior to humans—we are able to leverage hardware for artificial systems far superior to that accessible to humans. Chalmers talk was extremely thought-provoking; he structured the problem of consciousness in LLMs excellently—far better than I’ve seen to date, and as such was my favourite of the four.</p>
<p>I have linked to each of the talks, which are freely available to view above.</p>
<h3 id="references">References</h3>
<p>Fan, L.; Wang, G.; Jiang, Y.; Mandlekar, A.; Yang, Y.; Zhu, H.; Tang, A.; Huang, D.-A.; Zhu, Y.; and Anandkumar, A. 2022. Minedojo: Building open-ended embodied agents with</p>
<p>internet-scale knowledge. Advances in neural information processing systems, 35.</p>
<p>Henaff, M.; Raileanu, R.; Jiang, M.; and Rockt  ̈aschel, T. 2022. Exploration via Elliptical Episodic Bonuses. Advances in neural information processing systems, 35.</p>
<p>Humphreys, P. C.; Guez, A.; Tieleman, O.; Sifre, L.; Weber, T.; and Lillicrap, T. 2022. Large-Scale Retrieval for Reinforcement Learning. Advances in neural information processing systems, 35.</p>
<p>Lee, K.-H.; Nachum, O.; Yang, M.; Lee, L.; Freeman, D.; Xu, W.; Guadarrama, S.; Fischer, I.; Jang, E.; Michalewski, H.; et al. 2022. Multi-game decision transformers. Advances in neural information processing systems, 35.</p>
<p>Paster, K.; McIlraith, S.; and Ba, J. 2022. You Can’t Count on Luck: Why Decision Transformers Fail in Stochastic Environments. Advances in neural information processing systems, 35.</p>
<p>Schaul, T.; Barreto, A.; Quan, J.; and Ostrovski, G. 2022. The phenomenon of policy churn. Advances in neural information processing systems, 35.</p>
<p>Touati, A.; Rapin, J.; and Ollivier, Y. 2022. Does Zero-Shot Reinforcement Learning Exist?</p>
]]></content>
        </item>
        
        <item>
            <title>One Hour RL</title>
            <link>https://enjeeneer.io/posts/2022/02/one-hour-rl/</link>
            <pubDate>Fri, 25 Feb 2022 15:23:20 +0000</pubDate>
            
            <guid>https://enjeeneer.io/posts/2022/02/one-hour-rl/</guid>
            <description>An Introduction to Reinforcement Learning Tom Bewley &amp;amp; Scott Jeen Alan Turing Institute 24/02/2022 The best way to walk through this tutorial is using the accompanying Jupyter Notebook:

[Jupyter Notebook]
1 | Markov Decision Processes: A Model of Sequential Decision Making 1.1. MDP (semi-)Formalism In reinforcement learning (RL), an agent takes actions in an environment to change its state over discrete timesteps $t$, with the goal of maximising the future sum of a scalar quantity known as reward.</description>
            <content type="html"><![CDATA[<h1 id="an-introduction-to-reinforcement-learning">An Introduction to Reinforcement Learning</h1>
<h2 id="tom-bewleyhttpstombewleycom----scott-jeenhttpsenjeeneerio"><a href="https://tombewley.com/">Tom Bewley</a>  &amp;  <a href="https://enjeeneer.io/">Scott Jeen</a></h2>
<h2 id="alan-turing-institute">Alan Turing Institute</h2>
<h3 id="24022022">24/02/2022</h3>
<p>The best way to walk through this tutorial is using the accompanying Jupyter Notebook:</p>
<p><a href="http://colab.research.google.com/github/enjeeneer/talks/blob/main/2021-11-17-RISEPresentations/notebook.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></p>
<p>[<a href="http://nbviewer.jupyter.org/github/enjeeneer/talks/blob/main/2021-11-17-RISEPresentations/notebook.ipynb">Jupyter Notebook</a>]</p>
<h1 id="1--markov-decision-processes-a-model-of-sequential-decision-making">1 | Markov Decision Processes: A Model of Sequential Decision Making</h1>
<h2 id="11-mdp-semi-formalism">1.1. MDP (semi-)Formalism</h2>
<p>In reinforcement learning (RL), an <em>agent</em> takes <em>actions</em> in an <em>environment</em> to change its state over discrete timesteps $t$, with the goal of maximising the future sum of a scalar quantity known as <em>reward</em>. We formalise this interaction as an agent-environment loop, mathematically described as a Markov Decision Process (MDP).</p>
<img src='https://github.com/enjeeneer/sutton_and_barto/blob/main/images/chapter3_1.png?raw=true' width='700'>
<p>MDPs break the I.I.D. data assumption of supervised and unsupervised learning; the agent <em>causally influences</em> the data it sees through its choice of actions. However, one assumption we do make is the <em>Markov property</em>, which says that the state representation captures <em>all relevent information</em> from the past. Formally, state transitions depend only on the most recent state and action,
$$
\mathbb{P}[S_{t+1} | S_1,A_1 \ldots, S_t,A_t]=\mathbb{P}[S_{t+1} | S_t,A_t],
$$
and rewards depend only on the most recent transition,
$$
\mathbb{P}[R_{t+1} | S_1,A_1 \ldots, S_t,A_t,S_{t+1}] = \mathbb{P}[R_{t+1} | S_t,A_t,S_{t+1}].
$$</p>
<ul>
<li>Note: different sources use different notation here, but this is the most general.</li>
</ul>
<p>In some MDPs, a subset of states are designated as <em>terminal</em> (or <em>absorbing</em>). The agent-environment interaction loop ceases once a terminal state is reached, and restarts again at $t=0$ by sampling an state from an initialisation distribution $S_0\sim\mathbb{P}_\text{init}$. Such MDPs are known as <em>episodic</em>, while those without terminal states are known as <em>continuing</em>.</p>
<p>The goal of an RL agent is to pick actions that maximise the discounted cumulative sum of future rewards, also known as the <em>return</em> $G_t$:
$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots + \gamma^{T-t-1}R_{T},
$$
where $\gamma\in[0,1]$ is a discount factor and $T$ is the time of termination ($\infty$ in continuing MDPs).</p>
<p>To do so, it needs the ability to forecast the reward-getting effect of taking each action $A$ in each state $S$, potentially many timesteps into the future. This <em>temporal credit assignment</em> problem is one of the key factors that makes RL so challenging.</p>
<p>Before we go on, it&rsquo;s worth reflecting on how general the MDP formulation is. An extremely large class of problems can be cast as MDPs (it&rsquo;s even possible to represent supervised learning as a special case), and <a href="https://reader.elsevier.com/reader/sd/pii/S0004370221000862?token=3A56DFC12064E559FBF2F53CBE7A85E4E4BE24160CC0B9DDDAE18351D2FE61DA3BF02167A8FCAE3398396BBBEDFDA7A9&amp;originRegion=eu-west-1&amp;originCreation=20220224085155">this recent DeepMind paper</a> goes as far as to say that <em>all aspects of general intelligence</em> can be understood as serving the maximisation of future reward. Although not everybody agrees, this attitude motivates the heavy RL focus at organisations like DeepMind and OpenAI.</p>
<h2 id="12-mdp-example">1.2 MDP Example</h2>
<p>Here&rsquo;s a simple MDP (courtesy of David Silver @ DeepMind/UCL), which we&rsquo;ll be using throughout this course.</p>
<ul>
<li>White circle: non-terminal state</li>
<li>White square: terminal state</li>
<li>Black circle: action</li>
<li><span style="color:green">Green:</span> reward (depends only on $S_{t+1}$ here)</li>
<li><span style="color:blue">Blue:</span> state transition probability</li>
<li><span style="color:red">Red:</span> action probability for an exemplar policy</li>
<li>Note: edges with probability $1$ are unlabelled</li>
</ul>
<img src='https://github.com/tombewley/one-hour-rl/blob/main/images/student-mdp.svg?raw=true' width='700'>
<h2 id="13-open-ai-gym">1.3 Open AI Gym</h2>
<p><a href="https://gym.openai.com/">Open AI Gym</a> provides a unified framework for testing and comparing RL algorithms in Python, and offers a suite of MDPs that researchers can use to benchmark their work. It&rsquo;s important to be familiar with the conventions of Gym, because almost all modern RL code is built to work with it. Gym environment classes have two key methods:</p>
<ul>
<li><code>mdp.reset()</code>: reset the MDP to an initial state $S_0$ according to the initialisation distribution $\mathbb{P}_\text{init}$.</li>
<li><code>mdp.step(action)</code> : given an action $A_t$, combine with the current state $S_t$ to produce the next state according to $\mathbb{P}[S_{t+1} | S_t,A_t]$ and a scalar reward according to $\mathbb{P}[R_{t+1} | S_t,A_t,S_{t+1}]$.</li>
</ul>
<p>A Gym-compatible class for the student MDP shown above can be found in <code>mdp.py</code> in this repository. Let&rsquo;s import it now and explore what it can do!</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> mdp <span style="color:#f92672">import</span> StudentMDP
mdp <span style="color:#f92672">=</span> StudentMDP()
</code></pre></div><p>Firstly, we&rsquo;ll have a look at the initialisation probabilities and the behaviour of <code>mdp.reset()</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">print(mdp<span style="color:#f92672">.</span>initial_probs())
mdp<span style="color:#f92672">.</span>reset()
print(mdp<span style="color:#f92672">.</span>state)
</code></pre></div><pre><code>{'Class 1': 1.0, 'Class 2': 0.0, 'Class 3': 0.0, 'Facebook': 0.0, 'Pub': 0.0, 'Pass': 0.0, 'Asleep': 0.0}
Class 1
</code></pre>
<p>Next, let&rsquo;s check which actions are available in this initial state, and the action-dependent transition probabilities $\mathbb{P}[S_{t+1}|\text{Class 1},A_t]$.</p>
<ul>
<li>Reminder: the Markov property dictates that transition probabilities depend <em>only</em> on the current state and action.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">print(mdp<span style="color:#f92672">.</span>action_space(mdp<span style="color:#f92672">.</span>state))
print(mdp<span style="color:#f92672">.</span>transition_probs(mdp<span style="color:#f92672">.</span>state, <span style="color:#e6db74">&#34;Study&#34;</span>))
print(mdp<span style="color:#f92672">.</span>transition_probs(mdp<span style="color:#f92672">.</span>state, <span style="color:#e6db74">&#34;Go on Facebook&#34;</span>))
</code></pre></div><pre><code>{'Study', 'Go on Facebook'}
{'Class 2': 1.0}
{'Facebook': 1.0}
</code></pre>
<p>Calling <code>mdp.step(action)</code> samples and returns the next state $S_{t+1}$, alongside the reward $R_{t+1}$. Let&rsquo;s try calling this method repeatedly. What&rsquo;s happening here?</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">state, reward, _, _ <span style="color:#f92672">=</span> mdp<span style="color:#f92672">.</span>step(<span style="color:#e6db74">&#34;Study&#34;</span>) 
print(state, reward)
</code></pre></div><pre><code>Class 2 -2.0
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">mdp<span style="color:#f92672">.</span>action_space(<span style="color:#e6db74">&#34;Pass&#34;</span>)
</code></pre></div><pre><code>{'Fall asleep'}
</code></pre>
<p>So far, we&rsquo;ve only seen <em>deterministic</em> transitions, but having a pint in the pub has a <em>stochastic</em> effect; the state goes to one of the three classes with specified probabilities.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">print(mdp<span style="color:#f92672">.</span>action_space(<span style="color:#e6db74">&#34;Pub&#34;</span>))
print(mdp<span style="color:#f92672">.</span>transition_probs(<span style="color:#e6db74">&#34;Pub&#34;</span>, <span style="color:#e6db74">&#34;Have a pint&#34;</span>))
</code></pre></div><pre><code>{'Have a pint'}
{'Class 1': 0.2, 'Class 2': 0.4, 'Class 3': 0.4}
</code></pre>
<p>In this state, the behaviour of <code>mdp.step(action)</code> changes between repeated calls, even for a constant action.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">mdp<span style="color:#f92672">.</span>state <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Pub&#34;</span> <span style="color:#75715e"># Note that we&#39;re resetting the state to Pub each time</span>
state, reward, _, _ <span style="color:#f92672">=</span> mdp<span style="color:#f92672">.</span>step(<span style="color:#e6db74">&#34;Have a pint&#34;</span>)
print(state, reward)
</code></pre></div><pre><code>Class 2 -2.0
</code></pre>
<p>This MDP has just one terminal state.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">print(mdp<span style="color:#f92672">.</span>terminal_states())
</code></pre></div><pre><code>{'Asleep'}
</code></pre>
<p><code>mdp.step(action)</code> also returns a binary <code>done</code> flag, which is set to <code>True</code> if $S_{t+1}$ is a terminal state.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">mdp<span style="color:#f92672">.</span>state <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Class 2&#34;</span> 
state, reward, done, _ <span style="color:#f92672">=</span> mdp<span style="color:#f92672">.</span>step(<span style="color:#e6db74">&#34;Fall asleep&#34;</span>)
print(state, reward, done)

mdp<span style="color:#f92672">.</span>state <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Pass&#34;</span> 
state, reward, done, _ <span style="color:#f92672">=</span> mdp<span style="color:#f92672">.</span>step(<span style="color:#e6db74">&#34;Fall asleep&#34;</span>)
print(state, reward, done)
</code></pre></div><pre><code>Asleep 0.0 True
Asleep 0.0 True
</code></pre>
<p>Now let&rsquo;s bring an agent into the mix, and give it the exemplar policy shown in the diagram above.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> agent <span style="color:#f92672">import</span> Agent
agent <span style="color:#f92672">=</span> Agent(mdp) 
agent<span style="color:#f92672">.</span>policy <span style="color:#f92672">=</span> {
    <span style="color:#e6db74">&#34;Class 1&#34;</span>:  {<span style="color:#e6db74">&#34;Study&#34;</span>: <span style="color:#ae81ff">0.5</span>, <span style="color:#e6db74">&#34;Go on Facebook&#34;</span>: <span style="color:#ae81ff">0.5</span>},
    <span style="color:#e6db74">&#34;Class 2&#34;</span>:  {<span style="color:#e6db74">&#34;Study&#34;</span>: <span style="color:#ae81ff">0.8</span>, <span style="color:#e6db74">&#34;Fall asleep&#34;</span>: <span style="color:#ae81ff">0.2</span>},
    <span style="color:#e6db74">&#34;Class 3&#34;</span>:  {<span style="color:#e6db74">&#34;Study&#34;</span>: <span style="color:#ae81ff">0.6</span>, <span style="color:#e6db74">&#34;Go to the pub&#34;</span>: <span style="color:#ae81ff">0.4</span>},
    <span style="color:#e6db74">&#34;Facebook&#34;</span>: {<span style="color:#e6db74">&#34;Keep scrolling&#34;</span>: <span style="color:#ae81ff">0.9</span>, <span style="color:#e6db74">&#34;Close Facebook&#34;</span>: <span style="color:#ae81ff">0.1</span>},
    <span style="color:#e6db74">&#34;Pub&#34;</span>:      {<span style="color:#e6db74">&#34;Have a pint&#34;</span>: <span style="color:#ae81ff">1.</span>},
    <span style="color:#e6db74">&#34;Pass&#34;</span>:     {<span style="color:#e6db74">&#34;Fall asleep&#34;</span>: <span style="color:#ae81ff">1.</span>},
    <span style="color:#e6db74">&#34;Asleep&#34;</span>:   {<span style="color:#e6db74">&#34;Stay asleep&#34;</span>: <span style="color:#ae81ff">1.</span>}}

</code></pre></div><p>We can query the policy in a similar way to the MDP&rsquo;s properties, and observe its stochastic behaviour.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">print(agent<span style="color:#f92672">.</span>policy[<span style="color:#e6db74">&#34;Class 1&#34;</span>])
print([agent<span style="color:#f92672">.</span>act(<span style="color:#e6db74">&#34;Class 1&#34;</span>) <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">20</span>)])
</code></pre></div><pre><code>{'Study': 0.5, 'Go on Facebook': 0.5}
['Go on Facebook', 'Study', 'Go on Facebook', 'Study', 'Go on Facebook', 'Study', 'Go on Facebook', 'Study', 'Go on Facebook', 'Study', 'Study', 'Study', 'Go on Facebook', 'Study', 'Study', 'Study', 'Go on Facebook', 'Go on Facebook', 'Go on Facebook', 'Study']
</code></pre>
<p>Bringing it all together:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">mdp<span style="color:#f92672">.</span>verbose <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>
state <span style="color:#f92672">=</span> mdp<span style="color:#f92672">.</span>reset()
done <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
<span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> done:
    state, reward, done, info <span style="color:#f92672">=</span> mdp<span style="color:#f92672">.</span>step(agent<span style="color:#f92672">.</span>act(state))
</code></pre></div><pre><code>=========================== EPISODE   2 ===========================
| Time  | State    | Action         | Reward | Next state | Done  |
|-------|----------|----------------|--------|------------|-------|
| 0     | Class 1  | Study          | -2.0   | Class 2    | False |
| 1     | Class 2  | Study          | -2.0   | Class 3    | False |
| 2     | Class 3  | Study          | 10.0   | Pass       | False |
| 3     | Pass     | Fall asleep    |  0.0   | Asleep     | True  |
</code></pre>
<p>How &ldquo;good&rdquo; is this policy? To answer this, we need to calculate its expected return.</p>
<h1 id="2--policy-evaluation-the-temporal-difference-method">2 | Policy Evaluation: The Temporal Difference Method</h1>
<p>For a policy $\pi$, the <em>Q value</em> $Q_\pi(S_t,A_t)$ is the expected return from taking action $A_t$ in state $S_t$, and following $\pi$ thereafter. It thus quantifies how well the policy can be expected to perform, starting from this state-action pair. Q values exhibit an elegant recursive relationship known as the <em>Bellman equation</em>:
$$
Q_\pi(S_t,A_t)=\sum_{S_{t+1}}\mathbb{P}[S_{t+1}|S_t,A_t]\left(\mathbb{E}[R_{t+1} | S_t,A_t,S_{t+1}]+\gamma\times\sum_{A_{t+1}}\pi(A_{t+1}|S_{t+1})\times Q_\pi(S_{t+1},A_{t+1})\right).
$$</p>
<p>i.e. <strong>The Q value for a state-action pair is equal to the immediate reward, plus the $\gamma$-discounted Q value for the <em>next</em> state-action pair, with expectations taken over both the transition function $\mathbb{P}$ and the policy $\pi$.</strong></p>
<p>This is a bit of a mouthful, but the Bellman equation is perhaps the single most important thing to understand if you really want to &ldquo;get&rdquo; reinforcement learning.</p>
<p>To gain some intuition for this relationship, here are estimated Q values for the exemplar policy in the student MDP. Here we&rsquo;re using a discount factor of $\gamma=0.95$</p>
<ul>
<li>Note that these values are only approximate, so the Bellman equation doesn&rsquo;t hold exactly!</li>
</ul>
<img src='https://github.com/tombewley/one-hour-rl/blob/main/images/student-mdp-Q-values.svg?raw=true' width='700'>
<p>To take one example:
$$
Q(\text{Class 2},\text{Study})=-2+0.95\times [(0.6\times Q(\text{Class 3},\text{Study})+0.4\times Q(\text{Class 3},\text{Go to the pub}))]
$$
$$
=-2+0.95\times[(0.6\times 9.99+0.4\times 1.81)]
$$
$$
=4.38\approx 4.36
$$</p>
<p>How did we arrive at these Q value estimates? Here&rsquo;s where the real magic happens.</p>
<p>The <em>Bellman backup</em> algorithm makes use of this recursive relationship to update the Q value for a state-action pair based on the <em>current estimate of the value for the next state</em>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">GAMMA <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.95</span>  <span style="color:#75715e"># Discount factor</span>
ALPHA <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.001</span> <span style="color:#75715e"># Learning rate</span>

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">bellman_backup</span>(agent, state, action, reward, next_state, done):

    Q_next <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.</span> <span style="color:#66d9ef">if</span> done <span style="color:#66d9ef">else</span> agent<span style="color:#f92672">.</span>Q[next_state][agent<span style="color:#f92672">.</span>act(next_state)]

    agent<span style="color:#f92672">.</span>Q[state][action] <span style="color:#f92672">+=</span> ALPHA <span style="color:#f92672">*</span> ( reward <span style="color:#f92672">+</span> GAMMA <span style="color:#f92672">*</span> Q_next <span style="color:#f92672">-</span> agent<span style="color:#f92672">.</span>Q[state][action])
</code></pre></div><p>By sampling episodes in our MDP using the current policy we can collect rewards and update our Q-function accordingly. The algorithm we use to evaluate policies is called policy evaluation, and it uses the Bellman back-up which has two hyperparameters $\gamma$ and $\alpha$. $\gamma$ is the discount factor that</p>
<p>Import the MDP and define the policy again.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> mdp <span style="color:#f92672">import</span> StudentMDP
<span style="color:#f92672">from</span> agent <span style="color:#f92672">import</span> Agent
mdp <span style="color:#f92672">=</span> StudentMDP(verbose<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
agent <span style="color:#f92672">=</span> Agent(mdp) 
agent<span style="color:#f92672">.</span>policy <span style="color:#f92672">=</span> {
    <span style="color:#e6db74">&#34;Class 1&#34;</span>:  {<span style="color:#e6db74">&#34;Study&#34;</span>: <span style="color:#ae81ff">0.5</span>, <span style="color:#e6db74">&#34;Go on Facebook&#34;</span>: <span style="color:#ae81ff">0.5</span>},
    <span style="color:#e6db74">&#34;Class 2&#34;</span>:  {<span style="color:#e6db74">&#34;Study&#34;</span>: <span style="color:#ae81ff">0.8</span>, <span style="color:#e6db74">&#34;Fall asleep&#34;</span>: <span style="color:#ae81ff">0.2</span>},
    <span style="color:#e6db74">&#34;Class 3&#34;</span>:  {<span style="color:#e6db74">&#34;Study&#34;</span>: <span style="color:#ae81ff">0.6</span>, <span style="color:#e6db74">&#34;Go to the pub&#34;</span>: <span style="color:#ae81ff">0.4</span>},
    <span style="color:#e6db74">&#34;Facebook&#34;</span>: {<span style="color:#e6db74">&#34;Keep scrolling&#34;</span>: <span style="color:#ae81ff">0.9</span>, <span style="color:#e6db74">&#34;Close Facebook&#34;</span>: <span style="color:#ae81ff">0.1</span>},
    <span style="color:#e6db74">&#34;Pub&#34;</span>:      {<span style="color:#e6db74">&#34;Have a pint&#34;</span>: <span style="color:#ae81ff">1.</span>},
    <span style="color:#e6db74">&#34;Pass&#34;</span>:     {<span style="color:#e6db74">&#34;Fall asleep&#34;</span>: <span style="color:#ae81ff">1.</span>},
    <span style="color:#e6db74">&#34;Asleep&#34;</span>:   {<span style="color:#e6db74">&#34;Stay asleep&#34;</span>: <span style="color:#ae81ff">1.</span>}
}
</code></pre></div><p>Initially, we set all Q values to zero (this is actually arbitrary).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">agent<span style="color:#f92672">.</span>Q
</code></pre></div><pre><code>{'Class 1': {'Study': 0.0, 'Go on Facebook': 0.0},
 'Class 2': {'Study': 0.0, 'Fall asleep': 0.0},
 'Class 3': {'Study': 0.0, 'Go to the pub': 0.0},
 'Facebook': {'Keep scrolling': 0.0, 'Close Facebook': 0.0},
 'Pub': {'Have a pint': 0.0},
 'Pass': {'Fall asleep': 0.0},
 'Asleep': {}}
</code></pre>
<p>Run a single episode to see what happens.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">state <span style="color:#f92672">=</span> mdp<span style="color:#f92672">.</span>reset()
done <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
<span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> done:
    action <span style="color:#f92672">=</span> agent<span style="color:#f92672">.</span>act(state)
    next_state, reward, done, _ <span style="color:#f92672">=</span> mdp<span style="color:#f92672">.</span>step(action)
    
    print(<span style="color:#e6db74">&#39;Current action value:&#39;</span>, agent<span style="color:#f92672">.</span>Q[state][action])
    print(<span style="color:#e6db74">&#39;Reward obtained:&#39;</span>, reward)
    print(<span style="color:#e6db74">&#39;Next action value:&#39;</span>, <span style="color:#ae81ff">0.</span> <span style="color:#66d9ef">if</span> done <span style="color:#66d9ef">else</span> agent<span style="color:#f92672">.</span>Q[next_state][agent<span style="color:#f92672">.</span>act(next_state)])

    bellman_backup(agent, state, action, reward, next_state, done)

    print(<span style="color:#e6db74">&#39;Updated action value:&#39;</span>, agent<span style="color:#f92672">.</span>Q[state][action])
    print(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)

    state <span style="color:#f92672">=</span> next_state
</code></pre></div><pre><code>=========================== EPISODE  51 ===========================
| Time  | State    | Action         | Reward | Next state | Done  |
|-------|----------|----------------|--------|------------|-------|
| 0     | Class 1  | Study          | -2.0   | Class 2    | False |
Current action value: 4.271800760689531
Reward obtained: -2.0
Next action value: 6.9926093317691915
Updated action value: 4.272171938794022


| 1     | Class 2  | Study          | -2.0   | Class 3    | False |
Current action value: 6.9926093317691915
Reward obtained: -2.0
Next action value: 9.999149294082697
Updated action value: 6.9931159142668005


| 2     | Class 3  | Study          | 10.0   | Pass       | False |
Current action value: 9.999149294082697
Reward obtained: 10.0
Next action value: 0.0
Updated action value: 9.999150144788615


| 3     | Pass     | Fall asleep    |  0.0   | Asleep     | True  |
Current action value: 0.0
Reward obtained: 0.0
Next action value: 0.0
Updated action value: 0.0
</code></pre>
<p>Repeating a bunch of times, we gradually converge.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">mdp<span style="color:#f92672">.</span>verbose <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>

print(<span style="color:#e6db74">&#39;Initial Q&#39;</span>)
print(agent<span style="color:#f92672">.</span>Q)

<span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">20000</span>):
    state <span style="color:#f92672">=</span> mdp<span style="color:#f92672">.</span>reset()
    done <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
    <span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> done:
        action <span style="color:#f92672">=</span> agent<span style="color:#f92672">.</span>act(state)
        next_state, reward, done, _ <span style="color:#f92672">=</span> mdp<span style="color:#f92672">.</span>step(action)
        bellman_backup(agent, state, action, reward, next_state, done)
        state <span style="color:#f92672">=</span> next_state

print(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)
print(<span style="color:#e6db74">&#39;Converged Q&#39;</span>)
print(agent<span style="color:#f92672">.</span>Q)
</code></pre></div><pre><code>Initial Q
{'Class 1': {'Study': -0.002, 'Go on Facebook': -0.001}, 'Class 2': {'Study': -0.002, 'Fall asleep': 0.0}, 'Class 3': {'Study': 0.01, 'Go to the pub': 0.0}, 'Facebook': {'Keep scrolling': -0.0049995000249993754, 'Close Facebook': -0.00200095}, 'Pub': {'Have a pint': 0.0}, 'Pass': {'Fall asleep': 0.0}, 'Asleep': {}}


Converged Q
{'Class 1': {'Study': 1.3750628761712569, 'Go on Facebook': -11.288976651525505}, 'Class 2': {'Study': 4.485658109648779, 'Fall asleep': 0.0}, 'Class 3': {'Study': 9.999996524778595, 'Go to the pub': 1.8953439336946862}, 'Facebook': {'Keep scrolling': -11.233042781986304, 'Close Facebook': -6.6761905244797735}, 'Pub': {'Have a pint': 0.9312667143217461}, 'Pass': {'Fall asleep': 0.0}, 'Asleep': {}}
</code></pre>
<p>Note that although the policy evaluation process is guaranteed to converge eventually (for simple MDPs!), we are likely to see some discrepencies between runs of finite length because of the role of randomness in the data collection process. Here are the results of five independent repeats:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">{<span style="color:#e6db74">&#39;Class 1&#39;</span>: {<span style="color:#e6db74">&#39;Study&#39;</span>: <span style="color:#ae81ff">1.2650695038546025</span>, <span style="color:#e6db74">&#39;Go on Facebook&#39;</span>: <span style="color:#f92672">-</span><span style="color:#ae81ff">11.30468184426212</span>}, <span style="color:#e6db74">&#39;Class 2&#39;</span>: {<span style="color:#e6db74">&#39;Study&#39;</span>: <span style="color:#ae81ff">4.407552596737938</span>, <span style="color:#e6db74">&#39;Fall asleep&#39;</span>: <span style="color:#ae81ff">0.0</span>}, <span style="color:#e6db74">&#39;Class 3&#39;</span>: {<span style="color:#e6db74">&#39;Study&#39;</span>: <span style="color:#ae81ff">9.99999695776742</span>, <span style="color:#e6db74">&#39;Go to the pub&#39;</span>: <span style="color:#ae81ff">1.8487809354712246</span>}, <span style="color:#e6db74">&#39;Facebook&#39;</span>: {<span style="color:#e6db74">&#39;Keep scrolling&#39;</span>: <span style="color:#f92672">-</span><span style="color:#ae81ff">11.258053618560483</span>, <span style="color:#e6db74">&#39;Close Facebook&#39;</span>: <span style="color:#f92672">-</span><span style="color:#ae81ff">6.489974573408375</span>}, <span style="color:#e6db74">&#39;Pub&#39;</span>: {<span style="color:#e6db74">&#39;Have a pint&#39;</span>: <span style="color:#ae81ff">0.9454014270087486</span>}, <span style="color:#e6db74">&#39;Pass&#39;</span>: {<span style="color:#e6db74">&#39;Fall asleep&#39;</span>: <span style="color:#ae81ff">0.0</span>}, <span style="color:#e6db74">&#39;Asleep&#39;</span>: {}}
{<span style="color:#e6db74">&#39;Class 1&#39;</span>: {<span style="color:#e6db74">&#39;Study&#39;</span>: <span style="color:#ae81ff">1.3338704627380917</span>, <span style="color:#e6db74">&#39;Go on Facebook&#39;</span>: <span style="color:#f92672">-</span><span style="color:#ae81ff">11.222578014516461</span>}, <span style="color:#e6db74">&#39;Class 2&#39;</span>: {<span style="color:#e6db74">&#39;Study&#39;</span>: <span style="color:#ae81ff">4.404498313710967</span>, <span style="color:#e6db74">&#39;Fall asleep&#39;</span>: <span style="color:#ae81ff">0.0</span>}, <span style="color:#e6db74">&#39;Class 3&#39;</span>: {<span style="color:#e6db74">&#39;Study&#39;</span>: <span style="color:#ae81ff">9.999996607231745</span>, <span style="color:#e6db74">&#39;Go to the pub&#39;</span>: <span style="color:#ae81ff">1.9330819535637127</span>}, <span style="color:#e6db74">&#39;Facebook&#39;</span>: {<span style="color:#e6db74">&#39;Keep scrolling&#39;</span>: <span style="color:#f92672">-</span><span style="color:#ae81ff">11.237574593720579</span>, <span style="color:#e6db74">&#39;Close Facebook&#39;</span>: <span style="color:#f92672">-</span><span style="color:#ae81ff">6.649035509952115</span>}, <span style="color:#e6db74">&#39;Pub&#39;</span>: {<span style="color:#e6db74">&#39;Have a pint&#39;</span>: <span style="color:#ae81ff">1.0198591832482675</span>}, <span style="color:#e6db74">&#39;Pass&#39;</span>: {<span style="color:#e6db74">&#39;Fall asleep&#39;</span>: <span style="color:#ae81ff">0.0</span>}, <span style="color:#e6db74">&#39;Asleep&#39;</span>: {}}
{<span style="color:#e6db74">&#39;Class 1&#39;</span>: {<span style="color:#e6db74">&#39;Study&#39;</span>: <span style="color:#ae81ff">1.255108027766012</span>, <span style="color:#e6db74">&#39;Go on Facebook&#39;</span>: <span style="color:#f92672">-</span><span style="color:#ae81ff">11.190843458457234</span>}, <span style="color:#e6db74">&#39;Class 2&#39;</span>: {<span style="color:#e6db74">&#39;Study&#39;</span>: <span style="color:#ae81ff">4.3028079916966</span>, <span style="color:#e6db74">&#39;Fall asleep&#39;</span>: <span style="color:#ae81ff">0.0</span>}, <span style="color:#e6db74">&#39;Class 3&#39;</span>: {<span style="color:#e6db74">&#39;Study&#39;</span>: <span style="color:#ae81ff">9.999996368375</span>, <span style="color:#e6db74">&#39;Go to the pub&#39;</span>: <span style="color:#ae81ff">1.692402249138645</span>}, <span style="color:#e6db74">&#39;Facebook&#39;</span>: {<span style="color:#e6db74">&#39;Keep scrolling&#39;</span>: <span style="color:#f92672">-</span><span style="color:#ae81ff">11.009224020468848</span>, <span style="color:#e6db74">&#39;Close Facebook&#39;</span>: <span style="color:#f92672">-</span><span style="color:#ae81ff">6.456279660637165</span>}, <span style="color:#e6db74">&#39;Pub&#39;</span>: {<span style="color:#e6db74">&#39;Have a pint&#39;</span>: <span style="color:#ae81ff">0.7467114530860179</span>}, <span style="color:#e6db74">&#39;Pass&#39;</span>: {<span style="color:#e6db74">&#39;Fall asleep&#39;</span>: <span style="color:#ae81ff">0.0</span>}, <span style="color:#e6db74">&#39;Asleep&#39;</span>: {}}
{<span style="color:#e6db74">&#39;Class 1&#39;</span>: {<span style="color:#e6db74">&#39;Study&#39;</span>: <span style="color:#ae81ff">1.2734946938741027</span>, <span style="color:#e6db74">&#39;Go on Facebook&#39;</span>: <span style="color:#f92672">-</span><span style="color:#ae81ff">11.328006914127434</span>}, <span style="color:#e6db74">&#39;Class 2&#39;</span>: {<span style="color:#e6db74">&#39;Study&#39;</span>: <span style="color:#ae81ff">4.256107269897298</span>, <span style="color:#e6db74">&#39;Fall asleep&#39;</span>: <span style="color:#ae81ff">0.0</span>}, <span style="color:#e6db74">&#39;Class 3&#39;</span>: {<span style="color:#e6db74">&#39;Study&#39;</span>: <span style="color:#ae81ff">9.99999635381211</span>, <span style="color:#e6db74">&#39;Go to the pub&#39;</span>: <span style="color:#ae81ff">1.74113336614775</span>}, <span style="color:#e6db74">&#39;Facebook&#39;</span>: {<span style="color:#e6db74">&#39;Keep scrolling&#39;</span>: <span style="color:#f92672">-</span><span style="color:#ae81ff">11.34039736455563</span>, <span style="color:#e6db74">&#39;Close Facebook&#39;</span>: <span style="color:#f92672">-</span><span style="color:#ae81ff">6.777709970724558</span>}, <span style="color:#e6db74">&#39;Pub&#39;</span>: {<span style="color:#e6db74">&#39;Have a pint&#39;</span>: <span style="color:#ae81ff">0.7694312629253455</span>}, <span style="color:#e6db74">&#39;Pass&#39;</span>: {<span style="color:#e6db74">&#39;Fall asleep&#39;</span>: <span style="color:#ae81ff">0.0</span>}, <span style="color:#e6db74">&#39;Asleep&#39;</span>: {}}
{<span style="color:#e6db74">&#39;Class 1&#39;</span>: {<span style="color:#e6db74">&#39;Study&#39;</span>: <span style="color:#ae81ff">1.2650695038546025</span>, <span style="color:#e6db74">&#39;Go on Facebook&#39;</span>: <span style="color:#f92672">-</span><span style="color:#ae81ff">11.30468184426212</span>}, <span style="color:#e6db74">&#39;Class 2&#39;</span>: {<span style="color:#e6db74">&#39;Study&#39;</span>: <span style="color:#ae81ff">4.407552596737938</span>, <span style="color:#e6db74">&#39;Fall asleep&#39;</span>: <span style="color:#ae81ff">0.0</span>}, <span style="color:#e6db74">&#39;Class 3&#39;</span>: {<span style="color:#e6db74">&#39;Study&#39;</span>: <span style="color:#ae81ff">9.99999695776742</span>, <span style="color:#e6db74">&#39;Go to the pub&#39;</span>: <span style="color:#ae81ff">1.8487809354712246</span>}, <span style="color:#e6db74">&#39;Facebook&#39;</span>: {<span style="color:#e6db74">&#39;Keep scrolling&#39;</span>: <span style="color:#f92672">-</span><span style="color:#ae81ff">11.258053618560483</span>, <span style="color:#e6db74">&#39;Close Facebook&#39;</span>: <span style="color:#f92672">-</span><span style="color:#ae81ff">6.489974573408375</span>}, <span style="color:#e6db74">&#39;Pub&#39;</span>: {<span style="color:#e6db74">&#39;Have a pint&#39;</span>: <span style="color:#ae81ff">0.9454014270087486</span>}, <span style="color:#e6db74">&#39;Pass&#39;</span>: {<span style="color:#e6db74">&#39;Fall asleep&#39;</span>: <span style="color:#ae81ff">0.0</span>}, <span style="color:#e6db74">&#39;Asleep&#39;</span>: {}}
</code></pre></div><pre><code>{'Class 1': {'Study': 1.2650695038546025,
  'Go on Facebook': -11.30468184426212},
 'Class 2': {'Study': 4.407552596737938, 'Fall asleep': 0.0},
 'Class 3': {'Study': 9.99999695776742, 'Go to the pub': 1.8487809354712246},
 'Facebook': {'Keep scrolling': -11.258053618560483,
  'Close Facebook': -6.489974573408375},
 'Pub': {'Have a pint': 0.9454014270087486},
 'Pass': {'Fall asleep': 0.0},
 'Asleep': {}}
</code></pre>
<p>Try with $\gamma=0$</p>
<h1 id="3--policy-improvement">3 | Policy Improvement</h1>
<img src='https://github.com/tombewley/one-hour-rl/blob/main/images/policy-improvement-2.PNG?raw=true' width='500'>
<p>Having evaluated our policy $\pi$, how can we go about obtaining a better one? This question is the heart of <em>policy improvement</em>, perhaps the fundamental concept of RL. Recall, when we performed policy evaluation we obtained the value of taking every action in every state. Thus, we can perform policy improvement readily by picking our current best estimate of the optimal action from each state &ndash; so-called <em>greedy</em> action selection. Once we&rsquo;ve obtained a new policy, we can evaluate it as before. Continually iterating between policy evaluation and policy improvement in this way, we are guarenteed to reach the optimal policy $\pi^*$ according to the policy improvement theorem.</p>
<h3 id="31--q-learning-combining-policy-evaluation-and-improvement">3.1 | Q-learning: Combining Policy Evaluation and Improvement</h3>
<img src='https://github.com/tombewley/one-hour-rl/blob/main/images/q-learning.png?raw=true' width='700'>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> mdp <span style="color:#f92672">import</span> StudentMDP
mdp <span style="color:#f92672">=</span> StudentMDP(verbose<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> agent <span style="color:#f92672">import</span> QLearningAgent
agent <span style="color:#f92672">=</span> QLearningAgent(mdp, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>)
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">NUM_EPS <span style="color:#f92672">=</span> <span style="color:#ae81ff">50</span>
mdp<span style="color:#f92672">.</span>ep <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
<span style="color:#66d9ef">while</span> mdp<span style="color:#f92672">.</span>ep <span style="color:#f92672">&lt;</span> NUM_EPS:
    state <span style="color:#f92672">=</span> mdp<span style="color:#f92672">.</span>reset()
    done <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
    <span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> done:
        action <span style="color:#f92672">=</span> agent<span style="color:#f92672">.</span>act(state)
        next_state, reward, done, info <span style="color:#f92672">=</span> mdp<span style="color:#f92672">.</span>step(action)
        agent<span style="color:#f92672">.</span>learn(state, action, reward, next_state, done)
        state <span style="color:#f92672">=</span> next_state

    print(<span style="color:#e6db74">&#34;Value function:&#34;</span>)
    print(agent<span style="color:#f92672">.</span>Q)
    print(<span style="color:#e6db74">&#34;Policy:&#34;</span>)
    print(agent<span style="color:#f92672">.</span>policy)
    print(<span style="color:#e6db74">&#34;Epsilon:&#34;</span>, agent<span style="color:#f92672">.</span>epsilon)
    
    agent<span style="color:#f92672">.</span>epsilon <span style="color:#f92672">=</span> max(agent<span style="color:#f92672">.</span>epsilon <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> (NUM_EPS<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>), <span style="color:#ae81ff">0</span>)
</code></pre></div><pre><code>=========================== EPISODE   1 ===========================
| Time  | State    | Action         | Reward | Next state | Done  |
|-------|----------|----------------|--------|------------|-------|
| 0     | Class 1  | Study          | -2.0   | Class 2    | False |
| 1     | Class 2  | Study          | -2.0   | Class 3    | False |
| 2     | Class 3  | Study          | 10.0   | Pass       | False |
| 3     | Pass     | Fall asleep    |  0.0   | Asleep     | True  |
Value function:
{'Class 1': {'Study': -0.4, 'Go on Facebook': 0.0}, 'Class 2': {'Study': -0.4, 'Fall asleep': 0.0}, 'Class 3': {'Study': 2.0, 'Go to the pub': 0.0}, 'Facebook': {'Keep scrolling': 0.0, 'Close Facebook': 0.0}, 'Pub': {'Have a pint': 0.0}, 'Pass': {'Fall asleep': 0.0}, 'Asleep': {}}
Policy:
{'Class 1': {'Study': 0.5, 'Go on Facebook': 0.5}, 'Class 2': {'Study': 0.5, 'Fall asleep': 0.5}, 'Class 3': {'Study': 0.5, 'Go to the pub': 0.5}, 'Facebook': {'Keep scrolling': 0.5, 'Close Facebook': 0.5}, 'Pub': {'Have a pint': 1.0}, 'Pass': {'Fall asleep': 1.0}, 'Asleep': {}}
Epsilon: 1.0

=========================== EPISODE  50 ===========================
| Time  | State    | Action         | Reward | Next state | Done  |
|-------|----------|----------------|--------|------------|-------|
| 0     | Class 1  | Study          | -2.0   | Class 2    | False |
| 1     | Class 2  | Study          | -2.0   | Class 3    | False |
| 2     | Class 3  | Study          | 10.0   | Pass       | False |
| 3     | Pass     | Fall asleep    |  0.0   | Asleep     | True  |
Value function:
{'Class 1': {'Study': 4.2185955736170015, 'Go on Facebook': -2.843986498540236}, 'Class 2': {'Study': 6.978887265282676, 'Fall asleep': 0.0}, 'Class 3': {'Study': 9.997403851570732, 'Go to the pub': 1.0297507967148403}, 'Facebook': {'Keep scrolling': -3.2301556459908016, 'Close Facebook': -0.8716820424598939}, 'Pub': {'Have a pint': 2.6089417712654472}, 'Pass': {'Fall asleep': 0.0}, 'Asleep': {}}
Policy:
{'Class 1': {'Study': 1.0, 'Go on Facebook': 0.0}, 'Class 2': {'Study': 1.0, 'Fall asleep': 0.0}, 'Class 3': {'Study': 1.0, 'Go to the pub': 0.0}, 'Facebook': {'Keep scrolling': 0.0, 'Close Facebook': 1.0}, 'Pub': {'Have a pint': 1.0}, 'Pass': {'Fall asleep': 1.0}, 'Asleep': {}}
Epsilon: 0
</code></pre>
<p>We find that after 50 episodes the agent has obtained the optimal policy $\pi_*$!</p>
<h1 id="4--deep-rl">4 | Deep RL</h1>
<p>So far, we&rsquo;ve tabularised the state-action space. Whilst useful for explaining the fundamental concepts that underpin RL, the real world state-action spaces are generally continuous and thus impossible to tabularise. To combat this, function approximators are used instead. In the past these included x, but more recently, deep neural networks have been used giving rise to the field of Deep Reinforcement Learning.</p>
<p>The seminal Deep RL algorithm is Deep Q Learning which uses neural networks to represent the $Q$ function. The network takes the current obervation $o_t$ as input and predicts the value of each action. The agent&rsquo;s policy is $\epsilon$-greedy as before i.e. it takes the value-maximising action with probability $1 - \epsilon$. Deep Q learning</p>
<p>Below, we run 500 episodes of the canonical Cartpole task using Deep Q learning. The agent&rsquo;s goal is to balance the pole in the upright position for as long as possible starting from an initially random position.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> gym
<span style="color:#f92672">from</span> dqn_agent <span style="color:#f92672">import</span> Agent
<span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">env <span style="color:#f92672">=</span> gym<span style="color:#f92672">.</span>make(<span style="color:#e6db74">&#39;CartPole-v1&#39;</span>)
agent <span style="color:#f92672">=</span> Agent(gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.99</span>, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, lr<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0001</span>, n_actions<span style="color:#f92672">=</span>env<span style="color:#f92672">.</span>action_space<span style="color:#f92672">.</span>n, input_dims<span style="color:#f92672">=</span>[env<span style="color:#f92672">.</span>observation_space<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]],
              mem_size<span style="color:#f92672">=</span><span style="color:#ae81ff">50000</span>, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>,  eps_dec<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-3</span>, eps_min<span style="color:#f92672">=</span><span style="color:#ae81ff">0.05</span>, replace<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>,
              env_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cartpole&#39;</span>, chkpt_dir<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;tmp/dqn&#39;</span>)

best_score <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>np<span style="color:#f92672">.</span>inf
episodes <span style="color:#f92672">=</span> <span style="color:#ae81ff">500</span>
scores, avg_score, eps_history <span style="color:#f92672">=</span> [], [], []

<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(episodes):
    score <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    done <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
    observation <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()
    env<span style="color:#f92672">.</span>render()
    <span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> done:
        action <span style="color:#f92672">=</span> agent<span style="color:#f92672">.</span>choose_action(observation)
        observation_, reward, done, info <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(action)
        score <span style="color:#f92672">+=</span> reward
        agent<span style="color:#f92672">.</span>store_transition(observation, action, reward, observation_, done)
        agent<span style="color:#f92672">.</span>learn()
        observation <span style="color:#f92672">=</span> observation_
        env<span style="color:#f92672">.</span>render()
    
    scores<span style="color:#f92672">.</span>append(score)
    eps_history<span style="color:#f92672">.</span>append(agent<span style="color:#f92672">.</span>epsilon)
    
    avg_score <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>mean(scores[<span style="color:#f92672">-</span><span style="color:#ae81ff">100</span>:])
    
    print(<span style="color:#e6db74">&#39;episode&#39;</span>, i, <span style="color:#e6db74">&#39;score </span><span style="color:#e6db74">%.2f</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> score, <span style="color:#e6db74">&#39;average score </span><span style="color:#e6db74">%.2f</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> avg_score)
</code></pre></div><h1 id="5--what-did-we-miss-out">5 | What Did We Miss Out?</h1>
<ul>
<li>Dynamic programming (when transition probabilities are known)</li>
<li>Monte Carlo</li>
<li>Exploration strategies</li>
<li>Continuous actions</li>
<li>Policy gradient, actor-critic</li>
<li>Model-based</li>
<li>Partial observability</li>
</ul>
<p>What next? RL interest group?</p>
]]></content>
        </item>
        
        <item>
            <title>Presenting with Jupyter Notebooks</title>
            <link>https://enjeeneer.io/posts/2021/11/presenting-with-jupyter-notebooks/</link>
            <pubDate>Wed, 17 Nov 2021 09:03:11 -0500</pubDate>
            
            <guid>https://enjeeneer.io/posts/2021/11/presenting-with-jupyter-notebooks/</guid>
            <description>The best way to walk through this tutorial is using the accompanying Jupyter Notebook: 
[Jupyter Notebook]
- In the last year I&amp;rsquo;ve started presenting work using Jupyter Notebooks, rebelling against the Bill Gates&#39;-driven status-quo. Here I&amp;rsquo;ll explain how to do it. It&amp;rsquo;s not difficult, but in my opinion makes presentations look slicker, whilst allowing you to run code live in a presentation if you like. First, we need to download the plug-in that gives us the presentation functionality, it&amp;rsquo;s called RISE.</description>
            <content type="html"><![CDATA[<h3 id="the-best-way-to-walk-through-this-tutorial-is-using-the-accompanying-jupyter-notebook">The best way to walk through this tutorial is using the accompanying Jupyter Notebook:</h3>
<p><a href="http://colab.research.google.com/github/enjeeneer/talks/blob/main/2021-11-17-RISEPresentations/notebook.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></p>
<p>[<a href="http://nbviewer.jupyter.org/github/enjeeneer/talks/blob/main/2021-11-17-RISEPresentations/notebook.ipynb">Jupyter Notebook</a>]</p>
<h3 id="-">-</h3>
<p>In the last year I&rsquo;ve started presenting work using Jupyter Notebooks, rebelling against the Bill Gates'-driven status-quo. Here I&rsquo;ll explain how to do it. It&rsquo;s not difficult, but in my opinion makes presentations look slicker, whilst allowing you to run code live in a presentation if you like. First, we need to download the plug-in that gives us the presentation functionality, it&rsquo;s called <a href="https://rise.readthedocs.io/en/stable/index.html">RISE</a>. We can do this easily using pip in a terminal window:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">pip install RISE
</code></pre></div><p>Once installed, our first move is to add the presentation toggles to our notebook cells. We do this by clicking <em>View</em> in the menu bar, then <em>Cell Toolbar</em>, then <em>Slideshow</em>:</p>
<h2 id="adding-presentation-toggles-to-cells">Adding Presentation Toggles to Cells</h2>
<figure><img src="https://github.com/enjeeneer/talks/blob/main/2021-11-17-RISEPresentations/images/slideshow_2.gif?raw=true"/>
</figure>

<h2 id="slide-types">Slide Types</h2>
<p>This adds a <code>Slide Type</code> dropdown to each cell in the notebook. Here we can choose one of five options:</p>
<ul>
<li><strong>Slide</strong>: Used to start a new chapter in your presentation, think of this as a section heading in LaTeX.</li>
<li><strong>Sub-slide</strong>: Slide falling within the chapter defined by a <strong>Slide</strong> cell.</li>
<li><strong>Fragment</strong>: this is to split the contents of one slide into pieces; a cell marked as a fragment will create a break inside the slide; it will not show up right away, you will need to press Space one more time to see it.</li>
<li><strong>Skip</strong>: Skips cell when in presenter mode.</li>
<li><strong>Notes</strong>: Cell that allows the author to write notes on a slide that aren&rsquo;t shown in presenter view.</li>
</ul>
<p>As with any notebook, we can define the cell type to be either <code>Markdown</code> or <code>Code</code>. As you&rsquo;d expect, we present any text or image-based slide in <code>Markdown</code>, reserving the <code>Code</code> cell type if and only if we want to explicitly run some code in the presentation. If you aren&rsquo;t familiar, Markdown is a straightforward language for text formatting; I won&rsquo;t go into the details here, but suffice to say you can learn the basics of Markdown in 5 minutes. You can find a useful cheatsheet <a href="https://www.markdownguide.org/cheat-sheet/">here</a>.</p>
<h2 id="images">Images</h2>
<p>Adding images is easy too. I advise creating a sub-directory in your working directory called <code>/images</code> and storing anything you want to present there. Then you display them in a markdown file using some simple HTML syntax:</p>
<p><code>&lt;img class=&quot;&quot; src=&quot;images/london_deaths.jpeg&quot; style=&quot;width:75%&quot;&gt;</code></p>
<p>You can manipulate the <code>style</code> attribute to change the size of the image. Don&rsquo;t worry, this is the only HTML you need to know!</p>
<figure><img src="https://github.com/enjeeneer/talks/blob/main/2021-11-17-RISEPresentations/images/london_deaths.jpeg?raw=true"/>
</figure>

<h2 id="entering-presentation-mode">Entering Presentation Mode</h2>
<p>To view your slideshow click on the bar-chart button in the menu bar. This will start the presentation from the cell currently selected:</p>
<figure><img src="https://github.com/enjeeneer/talks/blob/main/2021-11-17-RISEPresentations/images/start_show.gif?raw=true"/>
</figure>

<p>That&rsquo;s it! This tutorial has given you an introduction to the basics of RISE for presenting with Jupyter Notebooks, you can of course customise these to your heart&rsquo;s content using further plug-ins and more advanced Markdown. Here&rsquo;s a summary of the useful links from this document to finish:</p>
<ul>
<li><a href="https://rise.readthedocs.io/en/stable/index.html">RISE Documentation</a></li>
<li><a href="https://www.markdownguide.org/cheat-sheet/">Markdown Cheatsheet</a></li>
<li><a href="https://www.tablesgenerator.com/">Markdown Table Generator</a></li>
</ul>
<h1 id="thanks">Thanks!</h1>
<h3 id="twitter-enjeeneerhttpstwittercomenjeeneer">Twitter: <a href="https://twitter.com/enjeeneer">@enjeeneer</a></h3>
<h3 id="website-httpsenjeeneeriohttpsenjeeneerio">Website: <a href="https://enjeeneer.io/">https://enjeeneer.io/</a></h3>
]]></content>
        </item>
        
        <item>
            <title>Notes, Exercises and Code for Sutton and Barto&#39;s Reinforcement Learning: An Introduction (2018)</title>
            <link>https://enjeeneer.io/posts/2021/04/notes-exercises-and-code-for-sutton-and-bartos-reinforcement-learning-an-introduction-2018/</link>
            <pubDate>Fri, 30 Apr 2021 18:18:24 +0100</pubDate>
            
            <guid>https://enjeeneer.io/posts/2021/04/notes-exercises-and-code-for-sutton-and-bartos-reinforcement-learning-an-introduction-2018/</guid>
            <description>In the last few weeks I&amp;rsquo;ve been compiling a set of notes and exercise solutions for Sutton and Barto&amp;rsquo;s Reinforcement Learning: An Introduction. Admittedly, these were produced for my own benefit, but if you&amp;rsquo;d like to look at my notes, my (probably incorrect) answers to the exercises, or the code accommodating those answers, I&amp;rsquo;ll link directly to them below:
 Notes Exercises Code  Thanks to Bryn Hayder for inspiring this idea, and for providing his exercise solutions which helped me throughout.</description>
            <content type="html"><![CDATA[<p>In the last few weeks I&rsquo;ve been compiling a set of notes and exercise solutions for <a href="http://incompleteideas.net/book/RLbook2020.pdf">Sutton and Barto&rsquo;s Reinforcement Learning: An Introduction</a>. Admittedly, these were produced for my own benefit, but if you&rsquo;d like to look at my notes, my (probably incorrect) answers to the exercises, or the code accommodating those answers, I&rsquo;ll link directly to them below:</p>
<ul>
<li><a href="/sutton_and_barto/rl_notes.pdf"><strong>Notes</strong></a></li>
<li><a href="/sutton_and_barto/rl_exercises.pdf"><strong>Exercises</strong></a></li>
<li><a href="https://github.com/enjeeneer/sutton_and_barto"><strong>Code</strong></a></li>
</ul>
<p>Thanks to <a href="https://github.com/brynhayder">Bryn Hayder</a> for inspiring this idea, and for providing his exercise solutions which helped me throughout.</p>
]]></content>
        </item>
        
        <item>
            <title>Scott&#39;s Uncomprehensive Guide to Scotland </title>
            <link>https://enjeeneer.io/posts/2021/04/scotts-uncomprehensive-guide-to-scotland/</link>
            <pubDate>Mon, 19 Apr 2021 21:08:05 +0000</pubDate>
            
            <guid>https://enjeeneer.io/posts/2021/04/scotts-uncomprehensive-guide-to-scotland/</guid>
            <description>Hello treasured friend. If you&amp;rsquo;re reading this, it&amp;rsquo;s probably because I&amp;rsquo;ve force-fed you a link after discussing your upcoming trip to Scotland. I hope this is useful to you in some way. Think of this a travel guide that you can dip into when you find yourself in one of these places either hungry or bored. I don&amp;rsquo;t describe anything in detail, you&amp;rsquo;ll just have to take me on my word that these places are worth visiting.</description>
            <content type="html"><![CDATA[<p>Hello treasured friend. If you&rsquo;re reading this, it&rsquo;s probably because I&rsquo;ve force-fed you a link after discussing your upcoming trip to Scotland. I hope this is useful to you in some way. Think of this a travel guide that you can dip into when you find yourself in one of these places either hungry or bored. I don&rsquo;t describe anything in detail, you&rsquo;ll just have to take me on my word that these places are worth visiting.</p>
<p>Before diving in, I&rsquo;d like to make a quick overarching recommendation. If you only have one chance to visit, I would strongly advise using this visit to see Edinburgh in August for the <a href="https://en.wikipedia.org/wiki/Edinburgh_Festival_Fringe">Fringe Festival</a>. In general, you can&rsquo;t do better than Edinburgh; it&rsquo;s the capital, cultural centre, prettiest city; and in August you&rsquo;ll get the best weather. But the Fringe is a unique experience that I think should be on everyone&rsquo;s bucket list. Every pub, church, park, music venue (indeed, any open space) becomes a stage for artists, actors, and comedians. These artists range from world class to distinctly amateur, and the fun lies in booking a string of shows across an afternoon/evening, likely from performers you will never have heard of, and rolling the dice. You will see some awful shows that make you cringe, then you&rsquo;ll see a performance that will take your breath away leaving you in awe of their talent. The bars are open till 5am every day of the week (usually outlawed in Scotland), and the city is buzzing with excitement and energy. Edinburgh is the most cosmopolitan city in Scotland, but it is especially so for the Fringe, with  ~1 million tourists visiting. During my undergrad, I worked in different bars each Fringe I was there, so have seen plenty of it, and I promise you won&rsquo;t regret going, it&rsquo;s a special time.</p>
<p>Anyway, if you want to explore more than just the Fringe, here&rsquo;s some ideas of things to do. The two categories are: 1) Cities, 2) Special Events, and there&rsquo;s a little aside at the end on golf courses. I think the sub-structure is self-explanatory. I hope it&rsquo;s helpful!</p>
<h1 id="cities">Cities</h1>
<h2 id="edinburgh">Edinburgh</h2>
<h3 id="things-to-do-before-dark">Things to do before dark</h3>
<ul>
<li>Walk up Arthur&rsquo;s Seat via the Craggs for a view of the entire city <a href="https://goo.gl/maps/5Jpnucuso4ebNY5w9">[map]</a></li>
<li>Walk up Calton Hill for a view of Princess Street, the Castle and the Firth of Forth <a href="https://goo.gl/maps/o5Y5uA94EH9bRP7k8">[map]</a></li>
<li>The Castle (beware of the £20 entry fee if you&rsquo;re light on $$$ <a href="https://goo.gl/maps/WjrgDGNcJJFCV2BT6">[map]</a></li>
<li>Walk along the canal through Dean Village <a href="https://goo.gl/maps/oYKFGTvuNzDCuwWo9">[map]</a></li>
<li>Portobello beach (if you&rsquo;re feeling warm) <a href="https://goo.gl/maps/a9UsAs9WhXRBAVs37">[map]</a></li>
<li>Tour the university buildings; George Square/The Meadows <a href="https://goo.gl/maps/z9DmcxhgCNycQ3FY7">[map]</a> and Old College <a href="https://goo.gl/maps/StYFpYtcEzn9tFDY6">[map]</a></li>
</ul>
<h3 id="things-to-do-after-dark">Things to do after dark</h3>
<ul>
<li>Sneaky Pete&rsquo;s: playing techno/house, the best night in Edinburgh. Something happening most nights  <a href="https://goo.gl/maps/AVeVNFvc4JMPdLyr9">[map]</a></li>
<li>The Brass Monkey: an eccentric, unique locals bar <a href="https://goo.gl/maps/WEEQrNjGNGcRAvHy6">[map]</a></li>
<li>The Devil&rsquo;s Advocate: cool, sleek (and more expensive) city bar <a href="https://goo.gl/maps/19BmGMCQ5ya2pUTZ6">[map]</a></li>
<li>The Hanover Tap: easy-going student bar <a href="https://goo.gl/maps/CrXV6yB6fFTojF1i8">[map]</a></li>
<li>99 Hanover St.: cool city bar with the occasional DJ <a href="https://goo.gl/maps/AXt3AW6LXyuaFkJv8">[map]</a></li>
<li>Garibadli&rsquo;s: wild club near many of the above bars. Order the Gari&rsquo;s special at the bar! <a href="https://goo.gl/maps/CZPhQmjGVhzumwsq6">[map]</a></li>
<li>The Hanging Bat: cool bar with loads of beer <a href="https://g.page/thehangingbat?share">[map]</a></li>
</ul>
<h3 id="things-to-do-if-you-have-a-car">Things to do if you have a car</h3>
<ul>
<li>Visit North Berwick and Gullane, two lovely seaside towns along the coast <a href="https://goo.gl/maps/1r1kviJPhEbi3NWd9">[map]</a></li>
<li>The Pentland Hills <a href="https://goo.gl/maps/NVPLxfiqaNj9Bb9Z7">[map]</a></li>
</ul>
<h3 id="things-to-do-if-hungover">Things to do if hungover</h3>
<ul>
<li>Meltmongers (grilled cheese) <a href="https://goo.gl/maps/m8saRkuZyTyLKSNGA">[map]</a></li>
<li>Snax Cafe: infamous hangover food for pennies <a href="https://goo.gl/maps/X3nVXRehDAzP4yYe7">[map]</a></li>
<li>Wings: chicken wings in an unreasonable number of seasonings <a href="https://goo.gl/maps/h2AvnUGcYPzaSWmm6">[map]</a></li>
</ul>
<h3 id="lunch">Lunch</h3>
<ul>
<li>Nile Valley Cafe–me and my mates' favourite spot in the city (Sudanese Wraps) <a href="https://goo.gl/maps/xGS9on9vymkHhN3Q8">[map]</a></li>
<li>Tupiniquim (Brazilian crepes) <a href="https://goo.gl/maps/UhaxA5MDUTUgnVk58">[map]</a></li>
<li>J Reid Sandwich Shop: great salads <a href="https://goo.gl/maps/K6vCQJuafghiuHEY9">[map]</a></li>
<li>Victor Hugo Deli <a href="https://goo.gl/maps/m8saRkuZyTyLKSNGA">[map]</a></li>
<li>10 to 10 in Delhi (Indian) <a href="https://goo.gl/maps/zSGp67zvi61DGHvr5">[map]</a></li>
<li>The Scran and Scallie: up-market scottish gastropub. Some of the best pub food in the city <a href="https://goo.gl/maps/19BmGMCQ5ya2pUTZ6">[map]</a></li>
</ul>
<h3 id="dinner">Dinner</h3>
<ul>
<li>Yene Meze (Greek) <a href="https://goo.gl/maps/kW8YaB4f3j16qzPS9">[map]</a></li>
<li>Fishers in the City: delicious, up-market fish restaurant <a href="https://g.page/Fishers-in-the-city?share">[map]</a></li>
<li>The Bon Vivant: fantastic French restaurant <a href="https://goo.gl/maps/CrXV6yB6fFTojF1i8">[map]</a></li>
<li>The Outsider: chill atmosphere, serving delicious mixed cuisine <a href="https://goo.gl/maps/mjmUh3S5vhSgXe8p6">[map]</a></li>
<li>The Grain Store: amazing scottish cuisine, albeit pricey <a href="https://goo.gl/maps/yYxhqMV8HnUM1zEq7">[map]</a></li>
</ul>
<h3 id="coffee">Coffee</h3>
<ul>
<li>Project Coffee <a href="https://goo.gl/maps/MtfGh3uCsGpapB7CA">[map]</a></li>
<li>Soderberg <a href="https://goo.gl/maps/5vTA9SNftMSLM3Jm9">[map]</a></li>
<li>Maison de Moggy: a cafe full of cats <a href="https://goo.gl/maps/iZ5c6j2bYDkbiomZ6">[map]</a></li>
</ul>
<h2 id="glasgow">Glasgow</h2>
<h3 id="things-to-do-before-dark-1">Things to do before dark</h3>
<ul>
<li>Kelvingrove Art Gallery: some lovely exhibitions inc. <a href="https://en.wikipedia.org/wiki/Christ_of_Saint_John_of_the_Cross">Dali&rsquo;s Christ of Saint John of the Cross</a> <a href="https://goo.gl/maps/eCvcpUcUE4Lmyq4n7">[map]</a></li>
<li>Walk around the West End: Byres Road, Botanic Gardens, University Avenue etc. <a href="https://goo.gl/maps/NMuBdWBtLwh2SpGq7">[map]</a></li>
<li>Take the <a href="https://en.wikipedia.org/wiki/Glasgow_Subway">Subway</a>!</li>
</ul>
<h3 id="things-to-do-after-dark-1">Things to do after dark</h3>
<ul>
<li>Ashton Lane (specifically <a href="brelbar.com">Brel</a> for a drink and the <a href="ubiquitouschip.co,uk">Ubiquitous Chip</a> for food) <a href="https://goo.gl/maps/QfYxVpMeGbRGYJ6i9">[map]</a></li>
<li>The Arlington (an edgy locals bar) <a href="https://goo.gl/maps/1RoyojPkp3qxRGbw8">[map]</a></li>
<li>Oran Mor (if you&rsquo;re out this area (the west end) beyond midnight, this is the only place that&rsquo;s open till 3am) <a href="https://goo.gl/maps/pAnWoRiYjB96EzHr9">[map]</a></li>
<li>Sub club (the longest running underground club in the world, the best spot for techno and a fun night) <a href="https://goo.gl/maps/e7xh4AKv7JnrXDcs5">[map]</a></li>
<li>SWG3: the clue is in the name <a href="https://goo.gl/maps/7rcZiL1TrakfAivP9">[map]</a></li>
</ul>
<h3 id="things-to-do-if-you-have-a-car-1">Things to do if you have a car</h3>
<ul>
<li>Loch Lomond and Conic Hill <a href="https://goo.gl/maps/RgDsXhviKvHKo5dy8">[map]</a></li>
<li>Climb Queen&rsquo;s view <a href="https://goo.gl/maps/RgDsXhviKvHKo5dy8">[map]</a></li>
<li>Glengoyne Distillery <a href="https://g.page/Glengoyne?share">[map]</a></li>
<li>Mugdock Country Park &amp; Castle <a href="https://goo.gl/maps/stpHPSLAe7geNMNJA">[map]</a></li>
</ul>
<h3 id="things-to-do-if-hungover-1">Things to do if hungover</h3>
<ul>
<li>The University Cafe: famous locals cafe that do a sick Scottish breakfast <a href="https://goo.gl/maps/BKQPKYN3LHzc7XNV9">[map]</a></li>
<li>Hyndland Cafe: greasy breakfast food for locals <a href="https://goo.gl/maps/c2iAw3iXAMkpUZaG6">[map]</a></li>
</ul>
<h3 id="lunch-1">Lunch</h3>
<ul>
<li>Epicures: Standard brunch food, delicious  <a href="https://goo.gl/maps/MvWLvNLRRMeqs7cG7">[map]</a></li>
<li>The Hanoi Bike Ship: easy vietnamese food <a href="https://goo.gl/maps/1RDdm1VdSJAJLVm2A">[map]</a></li>
</ul>
<h3 id="dinner-1">Dinner</h3>
<ul>
<li>Balbir&rsquo;s: I think it&rsquo;s the best curry I&rsquo;ve had <a href="https://goo.gl/maps/BhrHeMfitjsxrJWW6">[map]</a></li>
<li>La Vita Spuntini: delicious Italian tapas <a href="https://goo.gl/maps/wZAmsMbwzkm2Lig78">[map]</a></li>
<li>Pizza Magic: the best pizza I&rsquo;ve had in the city (also applies if hungover) <a href="https://goo.gl/maps/Sx9reyZYKhrJ1Vwk6">[map]</a></li>
<li>Stravaigin: gourmet Scottish cuisine <a href="https://goo.gl/maps/SWBM2vNa9LLaoWNv9">[map]</a></li>
</ul>
<h3 id="coffee-1">Coffee</h3>
<ul>
<li>Tchai-Ovna House of Tea: this isn&rsquo;t coffee, but don&rsquo;t worry about that just come here, drink their tea, smoke their shisha and play chess. It&rsquo;s a great spot. <a href="https://goo.gl/maps/S86eTVP8Dtqg91X49">[map]</a></li>
<li>Laboratorio Espresso: If you do need coffee, this place is pretty good <a href="https://goo.gl/maps/nYNoJb9aFYfWp7Ue6">[map]</a></li>
</ul>
<h2 id="st-andrews">St Andrews</h2>
<h3 id="things-to-do-before-dark-2">Things to do before dark</h3>
<ul>
<li>Have a go at the O.G. crazy golf: The Himalayas. Pitch up and pay a couple of quid at the window and they will give you a putters and balls <a href="https://goo.gl/maps/gv1NKrc7Cx7AnWRc6">[map]</a>.</li>
<li>Walk along west sands, where they filmed <a href="https://www.youtube.com/watch?v=TLbWBlB2aWA"><em>that</em></a> scene in Chariots of Fire <a href="https://goo.gl/maps/uepd8JwdH3uPxvSg7">[map]</a></li>
<li>Get a fudge donut from Fisher &amp; Donaldson <a href="https://goo.gl/maps/LKmFK2JMkuG4Qe3o7">[map]</a></li>
<li>Walk to east sands and along the pier where the students <a href="https://news.st-andrews.ac.uk/archive/students-take-part-in-traditional-st-andrews-pier-walk/">meet before the start of new academic year</a> <a href="https://goo.gl/maps/gYVkPDS62mF7HB7v6">[map]</a></li>
<li>If you&rsquo;re there on a Sunday, have a picnic on the 18th fairway of the old course. It&rsquo;s public land and there&rsquo;s no play on a Sunday, you&rsquo;ll meet lots of dog walkers. <a href="https://goo.gl/maps/f5RvVnwfjUP6eVzi9">[map]</a></li>
<li>If you play golf, clearly try and play the Old, if you can&rsquo;t get on there my next favourite is the New. <a href="https://goo.gl/maps/zE6EZNL9sHhhRdb36">[map]</a></li>
</ul>
<h3 id="things-to-do-after-dark-2">Things to do after dark</h3>
<ul>
<li>The Dumvegan: historic golf-themed pub <a href="https://goo.gl/maps/MxoD6SDWJJtyPhgD8">[map]</a></li>
<li>The Keys: the localist of local pubs <a href="https://goo.gl/maps/7mHbi7KXhN79o9i1A">[map]</a></li>
<li>The Vic: the only place playing music till late <a href="https://goo.gl/maps/zL8KbhBeEM2fGpwS7">[map]</a></li>
<li>The Jigger: sit and watch the golfers go by (nb: if you&rsquo;re a golfer, the Jigger challenge is to nip if after you finish the 17th and drink as many pints as you like in half an hour, then you have to play the 18th in fewer shots than the number of pints drunk) <a href="https://g.page/jigger-inn?share">[map]</a></li>
</ul>
<h3 id="things-to-do-if-you-have-a-car-2">Things to do if you have a car</h3>
<ul>
<li>Go to Elie, have lunch in <a href="https://goo.gl/maps/8XcwJtZ1PNRzUkAG9">The Ship</a> and watch some beach cricket <a href="https://goo.gl/maps/DsCgtdEQGtvQspzG9">[map]</a></li>
<li>Anstruther for the best fish and chips in Scotland! <a href="https://goo.gl/maps/L2q5eY9utz1xB8287">[map]</a></li>
<li>Have a walk around the colourful houses in Pittenweem <a href="https://goo.gl/maps/rtaXBFoxczuNoSRM6">[map]</a></li>
</ul>
<h3 id="things-to-do-if-hungover-2">Things to do if hungover</h3>
<ul>
<li>Munch: cheap, delicious greasy food <a href="https://goo.gl/maps/QQ21ntM7VCJkWYB67">[map]</a></li>
<li>Toastie Bar: 50p toasties, ideal <a href="https://goo.gl/maps/jT3Ki1WT6nDnoynV6">[map]</a></li>
</ul>
<h3 id="lunch-2">Lunch</h3>
<ul>
<li>Forgan&rsquo;s: solid brunch menu <a href="https://g.page/ForgansSTA?share">[map]</a></li>
<li>CombiniCo: east asian cuisine, sushi etc. <a href="https://goo.gl/maps/jFsSwf7MxADigzTs9">[map]</a></li>
</ul>
<h3 id="dinner-2">Dinner</h3>
<ul>
<li>The Seafood Ristorante: amazing fresh fish from the harbour, pricey <a href="https://g.page/theSeafoodStA?share">[map]</a></li>
<li>The Rav: British cuisine, stylish surroundings <a href="https://goo.gl/maps/kCekTGKVYUNkn6v97">[map]</a></li>
</ul>
<h3 id="coffee-2">Coffee</h3>
<ul>
<li>Taste: my Uncle owns this place! <a href="https://goo.gl/maps/vysZ53CnTTrbBHL1A">[map]</a></li>
</ul>
<h1 id="special-events">Special Events</h1>
<h2 id="edinburgh-fringe">Edinburgh Fringe</h2>
<h3 id="venues-to-visit">Venues to visit</h3>
<ul>
<li>Pleasance Courtyard: high-quality comedy and drama and one of the cool old university buildings. The bars are sick too. <a href="https://goo.gl/maps/nh83Py5T7N5Xdbgc8">[map]</a></li>
<li>Gilded Balloon: Similar to Pleasance, really cool old building with great acts performing every year. <a href="https://goo.gl/maps/skjzcAoqYcY2XAJv8">[map]</a></li>
<li>The Stand: the best comedy club in Edinburgh <a href="https://goo.gl/maps/oCiiZpTNv5nDGqAM8">[map]</a></li>
<li>Underbelly Cowgate: a cool network of bars underneath the city, and a great spot to be for going out after a show <a href="https://goo.gl/maps/NaV4JpBcyueVZpkG7">[map]</a></li>
<li>Udderbelly: a large purple cow [map]](<a href="http://www.underbellyedinburgh.co.uk/#stq=&amp;stp=1">http://www.underbellyedinburgh.co.uk/#stq=&amp;stp=1</a>)</li>
</ul>
<h3 id="recurring-shows-to-see">Recurring shows to see</h3>
<ul>
<li>Late n' Live: on every night in the Gilded Balloon from 1am. Different comedians are selected each evening to perform a set. They&rsquo;ve already performed their usual, daily set earlier on, and have probably now had a few celebratory drinks. The crowd are equally loose and it creates a chaotic, hilarious atmosphere <a href="https://latenlive.co.uk/about/">[map]</a></li>
</ul>
<h2 id="north-coast-500">North Coast 500</h2>
<p>If you&rsquo;re planning a road trip through the Highlands then you&rsquo;re probably familiar with the North Coast 500. You can&rsquo;t go wrong following <a href="https://en.wikipedia.org/wiki/North_Coast_500#:~:text=The%20North%20Coast%20500%20is,Scotland%20in%20one%20touring%20route.">the intended route</a>, but here&rsquo;s a little advice on places I think you should definitely visit on the way.</p>
<ul>
<li>Glenfinnan Viaduct: If you&rsquo;ve seen Harry Potter then you&rsquo;ll recognise this as the bridge the train crosses en route to Hogwarts. It&rsquo;s beautiful, but works as a great spot to stop for lunch. <a href="https://goo.gl/maps/A5LmFpunQxGNgU9P9">[map]</a></li>
<li>The <a href="https://images.app.goo.gl/n6dZcDgixxPJad2r8">James Bond View</a> <a href="https://goo.gl/maps/Yvm6WqbqDW3HLLGZA">[map]</a></li>
<li>Eilean Donan Castle, also in James Bond lol <a href="https://goo.gl/maps/WQNE2ip1viDqgpux9">[map]</a></li>
<li>The prettiest bit of road to drive is near Applecross <a href="https://goo.gl/maps/gUFfQctdsUB9J168A">[map]</a></li>
<li>Get a flight to <a href="https://en.wikipedia.org/wiki/Barra_Airport">Barra</a> in the Outer Hebrides and land on the beach! <a href="https://goo.gl/maps/27GKka1FZMAGK8QX7">[map]</a></li>
<li>Isle of Harris, and specifically <a href="https://goo.gl/maps/wkoNLZcS1CpCdENM7">Luskentyre Beach</a>) <a href="https://goo.gl/maps/wkoNLZcS1CpCdENM7">[map]</a></li>
<li>Take the <a href="https://en.wikipedia.org/wiki/Westray_to_Papa_Westray_flight#:~:text=The%20Loganair%20Westray%20to%20Papa,fastest%20flight%20is%2053%20seconds.">shortest commercial flight</a> in the world from Westray to Papa Westray. It lasts 53 seconds. <a href="https://goo.gl/maps/AkH13qdFuG4KXXHB9">[map]</a></li>
</ul>
<h1 id="an-aside-on-golf-courses">An Aside on Golf Courses</h1>
<p>If you are into golf, here&rsquo;s my top 5 courses, and some hidden gems:</p>
<h2 id="top-5">Top 5</h2>
<ol>
<li>Royal Dornoch</li>
<li>Muirfield</li>
<li>Carnoustie</li>
<li>Troon</li>
<li>Kingsbarns</li>
</ol>
<p>I&rsquo;m yet to play Turnberry since the updates, but I&rsquo;m told it&rsquo;s now no. 1.</p>
<h2 id="hidden-gems-in-no-particular-order">Hidden Gems (in no particular order)</h2>
<ul>
<li>Ladybank</li>
<li>Moray</li>
<li>Nairn (if you consider it hidden)</li>
<li>Elie</li>
<li>Monifieth</li>
<li>Murcar</li>
<li>Shiskine</li>
<li>Luffness</li>
</ul>
<p>If you have any questions, please do send me an email. You can find my address on <a href="https://enjeeneer.io/">the homepage</a>.</p>
]]></content>
        </item>
        
        <item>
            <title>Golf, attention-spans, and degrowth</title>
            <link>https://enjeeneer.io/posts/2021/04/golf-attention-spans-and-degrowth/</link>
            <pubDate>Wed, 14 Apr 2021 16:10:24 +0100</pubDate>
            
            <guid>https://enjeeneer.io/posts/2021/04/golf-attention-spans-and-degrowth/</guid>
            <description>&amp;ldquo;It&amp;rsquo;s downwind, if he throws everything at it he&amp;rsquo;s got a chance of making the front edge&amp;rdquo; said Nick Faldo, CBS commentator and 6 time major champion, as Bryson DeChambeau ambled to the 6th tee at Bay Hill Country Club 5 weeks ago. DeChambeau was leading Arnold Palmer&amp;rsquo;s Bay Hill Invitational and considering his options from the tee of the 528 yard par 5. I say 528 yards, as that is the distance as offered by Bay Hill&amp;rsquo;s scorecard, but that is the distance measured around the lake blocking the player&amp;rsquo;s line from tee to green, but were the player to bypass the boomerang arc and aim directly for the green, the hole plays only 340 yards.</description>
            <content type="html"><![CDATA[<p>&ldquo;It&rsquo;s downwind, if he throws everything at it he&rsquo;s got a chance of making the front edge&rdquo; said Nick Faldo, CBS commentator and 6 time major champion, as Bryson DeChambeau ambled to the 6th tee at Bay Hill Country Club 5 weeks ago. DeChambeau was leading Arnold Palmer&rsquo;s Bay Hill Invitational and considering his options from the tee of the 528 yard par 5. I say 528 yards, as that is the distance as offered by Bay Hill&rsquo;s scorecard, but that is the distance measured around the lake blocking the player&rsquo;s line from tee to green, but were the player to bypass the boomerang arc and aim directly for the green, the hole plays only 340 yards.</p>
<p><figure><img src="/img/bayhill_6th.png" width="450" height="300"/>
</figure>

<em>Figure: The 6th hole at Bay Hill Country Club. The teebox is on the far left of the image and the green on the far right. Source: <a href="https://www.pgatour.com/tournaments/arnold-palmer-invitational-presented-by-mastercard.html">pgatour.com</a></em></p>
<p>DeChambeau had been building toward this moment since the first lockdown of 2020. He entered lockdown a lean 85kg athlete and, after four months of throwing metal around his gym, emerged 20kg heavier and several belt loops larger. DeChambeau had gambled on the idea that bulking up to quicken his swing speed and lengthen his drives would tip the statistics of golf in his favour. He assumed that the benefits granted by increased distance would outweigh the side affects of swinging the club faster. If he can approach the green with an 8 iron, he thinks, whilst the field are hitting 5 irons, he will outperform the better iron players than him, despite their increased skill. He was right to think so.  Since returning from lockdown DeChambeau is first in strokes gained off-the-tee on the US Tour (a measure of how much better he drives the ball compared to the field average), has won three times, has 17 top-10 finishes, and has cashed $10 million. At Bay Hill he drove to the periphery of the 6th green on the final day and won the tournament.</p>
<p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/G1kFhGapsto" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<em>Video: Bryson DeChambeau&rsquo;s drive on the 6th hole at Bay Hill Country Club on the third day of the Arnold Palmer Invitational</em></p>
<p>To a non-golfer this tactic may seem obvious, but the efficacy of this approach has been long debated. Many (many!) golfers have tried and failed to hit the ball further. Luke Donald, a slight Englishman, made it to world Number 1 in 2011 with a game based on subtlety and consistency; Martin Kaymer won two major championships with a swing that faded the ball–a curvature from left to right that tends to produce shorter drives. Both men changed their swings in an attempt to hit the ball further and have been unrecognisable since. The established thinking went thus: greater distance means reduced control with the benefit of one being balanced by the drawback of the other. Technology has tipped this balance. Since the mid-2000s equipment manufacturers have been producing ever-optimised clubs and balls. The clubs cater better to off-centre strikes (often leading to wayward shots in times past) and the balls fly straighter and further. Combined, players can swing much harder without fear that a miss-strike could send their drives into hazards or rough.</p>
<p>This creates two problems for the sport, which are 1) logistical and 2) (somewhat) philosophical. First, golf courses, usually built in the late 19th and early 20th centuries, are now too short for the professional game. As an attempted fix, tee boxes have been extended backward to lengthen holes and rough has been grown to penalise inaccuracy. However, this creates two further problems: 1a) courses are built on finite land and, in many places, their limits have been reached, and 1b) longer courses and thicker rough, somewhat ironically, exacerbate the advantage gained by distance, as it&rsquo;s easier to hit a shorter iron from the rough than a longer one. The second, philosophical issue, is that increased length eliminates what most see as the best aspects of the game. The accuracy-distance tradeoff has always been key to the sport: a player who was less powerful, but could control their ball (and their emotions) better than their opponent would emerge victorious. Moreover, longer, straighter shots allow players to cut corners, or fly second shots directly to the hole, when players from past eras would have to bend shots around corners or manage contours to get approach shots close.</p>
<p>It seems sensible, therefore, that corrective action should be taken, but the governing bodies are hesitant. There is a prevailing narrative, lobbied, I suspect, by TV executives and equipment manufacturers, that fans <em>want</em> to see the ball go further. Indeed, executives of these organisations appear to actively reject the science to further this agenda; Jay Monahan, commissioner of the US PGA Tour said in 2017 that trends in driving distance <a href="%5Bhttps://www.golfchannel.com/news/pga-tour-pga-america-issue-statements-study%5D(https://www.golfchannel.com/news/pga-tour-pga-america-issue-statements-study)">&ldquo;do not indicate a significant or abnormal increase in distance&rdquo;</a>. Average driving distance on the PGA Tour increased by <a href="%5Bhttps://www.pgatour.com/content/pgatour/stats/stat.101.y2017.html%5D(https://www.pgatour.com/content/pgatour/stats/stat.101.y2017.html)">14%</a> in the 15 years preceding that statement. Financial incentives make the motivations of the equipment manufacturers unsurprising, but I&rsquo;m staggered by the assertion that fans love distance. Do fans <em>really</em> want to see the ball go further? Like, <em>really</em>? Are we not discrediting the interests and intelligence of these fans? The PGA&rsquo;s primary riposte is that people&rsquo;s attention spans are getting shorter, <a href="https://www.bbc.co.uk/news/health-38896790">a debated assertion</a> (and topic for another post), and that if they don&rsquo;t catch viewer&rsquo;s attention early they will switch channel.</p>
<p>These concerns are not unique to golf&rsquo;s leadership. T20 cricket is now framed by fireworks and cheerleaders when 6s are hit; premier league football clubs play music after a goal is scored to boost the atmosphere; organisers of the Boat Race recently made the rowers walk from a misty dressing room as <a href="%5Bhttps://youtu.be/oi_Lo4QuhtU?t=2898%5D(https://youtu.be/oi_Lo4QuhtU?t=2898)">lasers buzzed around their heads</a>; and the Super Bowl&rsquo;s half-time show (famously) gets bigger every year. It seems unlikely that these efforts have made TV sport more popular today that in previous decades, so I looked up Super Bowl viewership data to investigate (see below). You&rsquo;ll note that roughly 75% of US TV owners tuned in for the 1967 edition, and that figure has remained largely stable ever since. Fans continue to enjoy the Super Bowl in similar proportions to decades past, despite efforts to make it appear more exciting or accessible.</p>
<p><figure><img src="/img/superbowl.png" width="700" height="300"/>
</figure>

<em>Figure: Super Bowl TV ratings 1967-2019. Source: <a href="https://en.wikipedia.org/wiki/Super_Bowl_television_ratings">Wikipedia: Super Bowl television ratings</a></em></p>
<p>I can&rsquo;t help but draw comparisons between golf&rsquo;s predicament and climate change. A clamour for growth that harms the environment; vested interests rejecting the facts; a misunderstanding of what people truly value. We know climate change has been driven by capitalist demands for growth, and we know these demands manifest as 2% annual increase targets to GDP, despite the plateauing of human well-being beyond <a href="%5Bhttps://ourworldindata.org/grapher/gdp-vs-happiness?xScale=linear&amp;minPopulationFilter=1000000&amp;time=2018%5D(https://ourworldindata.org/grapher/gdp-vs-happiness?xScale=linear&amp;minPopulationFilter=1000000&amp;time=2018)">$30,000 per capita</a>. With a finite planet, perennial growth, is, of course, a ridiculous idea. But more pressingly, growth in the next 30 years is incompatible with climate targets. Even if we deploy renewables as quickly as is hoped in best-case scenarios, we still have to reduce global energy use from 400EJ today to 240EJ by 2050 (<a href="%5Bhttps://www.nature.com/articles/s41560-018-0172-6%5D(https://www.nature.com/articles/s41560-018-0172-6)">Grubler et al (2018)</a>). We cannot make these reductions by efficiency improvements alone for several reasons, including <a href="https://bit.ly/3ddJWrf">Jevon&rsquo;s Paradox</a> and, in some cases, <a href="https://www.sciencedirect.com/science/article/pii/S0360544210000265">thermodynamic limits to energy efficiency</a>, so we have to find ways of reducing our energy demand in absolute terms. We have to find ways to degrow.</p>
<p>How can we convince our leaders, largely born during the late 20th century–the golden age of economic growth–that degrowth is a viable alternative? I think our best chance is to understand what maximises human well-being, show its separation from growth, and double down. In sport, we don&rsquo;t find joy in a 30 second Twitter highlight, but in the hours of subtlety and story that preceded it. When Ben Stokes flushes a 4 through the covers to beat the Aussies, it&rsquo;s not the shot itself that&rsquo;s exhilarating (to a non-Aussie!), it&rsquo;s the previous hour of blocking from the bespectacled Jack Leach, positioning our hero for victory, that we love. When Tiger Woods wins his 5th Masters, the joy comes from understanding his ten-year comeback, lifting himself from the depths of public disgrace to the heights of sporting nirvana–all for his children who never saw him in his prime. And when Bryson DeChambeau drives the 6th green at Bay Hill, fans cheer not because the ball goes far, but because they appreciate his graft through lockdown, and are excited to see his efforts vindicated. We derive fulfilment from depth, characters and stories, not from superficial tricks.</p>
<p>In life, too, our most fulfilling times are not the most expensive, the most energy-intensive or those that emit the most carbon. They are, in fact, the opposite. They&rsquo;re times spent laughing with friends, perfecting skills, exercising our creativity, connecting religiously, exploring the outdoors, feeling loved by others, and offering our love to others. All of these fulfilment-maximising activities come at low or no environmental cost. Crucially, they don&rsquo;t come after the Bank of England meets its fiscal targets for Q2. If our goal is a truly equitable, sustainable, <em>happy</em> world, then we must realise that growth, in the developed world, cannot provide it.</p>
<p>I suspect golf&rsquo;s governing bodies will soon change the golf ball to shorten drives, and stop the expansion of its historic stadia. Can we make the same changes to our economic systems?</p>
]]></content>
        </item>
        
        <item>
            <title>Book Review: Cradle-to-Cradle Remaking the Way We Make Things</title>
            <link>https://enjeeneer.io/posts/2018/11/book-review-cradle-to-cradle-remaking-the-way-we-make-things/</link>
            <pubDate>Tue, 20 Nov 2018 16:54:55 +0000</pubDate>
            
            <guid>https://enjeeneer.io/posts/2018/11/book-review-cradle-to-cradle-remaking-the-way-we-make-things/</guid>
            <description>Consider the cherry tree. A frenetic supply chain of blossom that produces thousands of seeds in the hope that one might fall to the soil, take root and grow. Its manufacturing model is inefficient, wasteful and laborious, yet beautifully nourishing to its surrounding ecology. The blossom that do not germinate instead provide nutrients for insects, microorganisms, animals and soil; its abundance and over-production should not be chastised, but celebrated. In Michael Braungart and William McDonough’s seminal book “Cradle-to-Cradle: Remaking the Way We Make Things”, the author’s ask: “What would the human-built world look like if it were built by a cherry tree?</description>
            <content type="html"><![CDATA[<figure><img src="/img/cradle.jpg" width="250" height="350"/>
</figure>

<p>Consider the cherry tree. A frenetic supply chain of blossom that produces thousands of seeds in the hope that one might fall to the soil, take root and grow. Its manufacturing model is inefficient, wasteful and laborious, yet beautifully nourishing to its surrounding ecology. The blossom that do not germinate instead provide nutrients for insects, microorganisms, animals and soil; its abundance and over-production should not be chastised, but celebrated. In Michael Braungart and William McDonough’s seminal book “Cradle-to-Cradle: Remaking the Way We Make Things”, the author’s ask: “What would the human-built world look like if it were built by a cherry tree?”. Braungart and McDonaugh assert that since the industrial revolution, humankind’s marriage with nature has been shaky and is now, in the 21st century, going through a messy, complicated divorce.</p>
<p>The authors argue that our conventional cradle-to-grave manufacturing and consumption processes are deeply flawed in their linearity and their contempt they show for the environment. They compel the reader to drive a personal and corporate paradigm shift toward a circular design economy – their so-called cradle-to-cradle (C2C) approach. Their manifesto for revolution asks that producers make three actions: one is to see waste as food for the environment; two is to separate biological and technical nutrients in products to ease ready recycling and upcycling; and three is to adopt eco-effective practices rather than eco-efficient practices that value and incorporate the environment in design. Central to the novel’s argument is the author’s criticism of western society’s obsession with death, indeed graves. They suggest that mankind is fixated on finding a grave for their products after their finite and often short useful life. Braungart and McDonough summarise their circular philosophy and set up their narrative by asking the reader “what if the industrial revolution had taken place in a society that believed in reincarnation? Would our cradle-to-grave approach become cradle-to-cradle?”</p>
<p>The concept of circularity was first discussed 52 years ago when Kenneth Boudling began questioning the linear openness of our economic activity in his piece “The Economics of the Coming Spaceship Earth” [1]. He identified society’s predisposition toward sending material to ‘sinks’ rather than reusing them in a closed economy. In 1989, this idea was further developed by Pearce and Kerry Turner when they first coined the phrase ‘circular economy’ [2],   after which Jackson synthesised much of the work in the field in his book “Material Concerns: Pollution, profit and quality of life” [3]. Thought toward bettering our products by copying natural design in the form of biomimicry was first discussed in 1982 by Merrill [4], before being popularised by Benyus in her seminal book “Biomimicry: Innovation Inspired by Nature” [5]. Cradle-to-Cradle’s call for ecological practice in industry and a closer alignment with nature draws heavily on these influences and preaches the author’s sermon in accessible English that resonates with academics, corporations and the public. In response to Cradle-to-Cradle, the Ellen Macarthur Foundation have furthered the shift toward a circular economy by producing a realisable framework that is founded on Braungart and McDonough’s cradle-to-cradle principles. The book has been cited thousands of times and has sold more than 300,000 copies worldwide. As such, it must be credited with starting a revolution in industrial design and in public discourse around societal overconsumption and waste.</p>
<p>Cradle-to-Cradle comments on various aspects of sustainability, notably: challenging orthodoxy through change, operating within environmental limits and enacting effective life cycle management. The novel-spanning motif that we must create a mindset shift toward waste equalling food is the clearest objection of orthodoxy. The authors are constantly challenging the idea of waste describing it as an artefact of human creation; “whatever humans make does not go ‘away’” they exclaim. They make the reader question where “away” really is; is a landfill “away”? Is a recycling plant “away”? Where is this abstract place? They emphasise that nothing enters or leaves the confines of our planet, thus we live in materially closed system. Braungart and McDonough state that “to eliminate the concept of waste means to design things – products, packaging and systems – from the very beginning on the understanding that waste does not exist”. Inoculating public and corporate minds with this message could be extremely powerful in catalysing the transition toward a sustainable circular economy and I believe this is the most influential, effective message from the book. The author’s exemplars of how we can make our waste nourishing and not damaging are often idealistic, particularly their assertion that we should design a car with clean water as effluent that could promote roadside ecology. Despite their head-in-the-clouds aspirations, I still feel this message is truly valuable. I would add that any efforts to shift the paradigm away from convention will inevitably seem utopic at first, but it is only by thinking laterally with great optimism that can we enact real change.</p>
<p>Throughout the text, Braungart and McDonaugh ask industry to design processes and products that operate within the environmental limits of the planet. They repeatedly remind the reader that “being less-bad is not good” referring to the environmental impact of our actions. They demonise so-called eco-efficiency, that is, for example: using less material in manufacturing, creating more carbon-efficient processes or using less water in agriculture. To that end, McDonough was quoted in an interview with the National Press Club as saying “If you want to go to Mexico, and you’re driving toward Canada, even if you slow down you’re still going to Canada”. Instead the authors ask the reader: “What would it mean to be 100% good?”, by that they mean operating with eco-effectiveness; designing systems so they are in deep symbiosis with nature. In doing so they say we can strive for a future of abundance, rather than one of limited eco-efficiency, bounded by the traditional environmentalist ideals of conservationism. The authors claim that their eco-effective principles will design systems that “pay back […] the environment with interest” and in my opinion this is where, their argument collapses.</p>
<p>Despite the authors giving thought to environmentalism at a systems level, I am not convinced the boundaries of their analysis are wide enough, and this particular utopic view is a clear example. It is thermodynamically impossible for a process to pay back the environment “with interest”, this would require it to add more energy to the environment than was taken in the first place. Sure, creating coffee cups that require few inputs to produce and can fertilise farmland when disposed would be excellent, and I welcome the innovation that cracks this perpetual motion-style problem, but these are impossibilities.</p>
<p>The author’s character assassination on eco-efficiency seems equally misguided. They state that we should build eco-effective buildings with green roofs that provide bird habitat and solar cooling, rather than designing more efficient air-conditioning systems or effective solar shades. Whilst I agree that green roofs bring positive benefits, the authors do not comment on the increased structural steel required to buttress these green roofs. What about the mineral extraction associated with this added steel? The carbon emissions produced in their manufacturing? The disposal of this steel at the building’s end of life? The bounds of their systems analysis appear blinkered. “Efficiency isn’t much fun” McDonough and Braungart assert. I would argue efficiency can be fun, as well as challenging and beneficial despite not paying back the environment with circular idealism. Strangely, the authors backtrack on their criticism of efficiency, concluding their comments by saying “This is not to condemn all efficiency. When implemented as a tool within a larger, effective system […] efficiency can actually be valuable”. Their lack of continuity in this argument is a damning indictment of their eco-effective propaganda.</p>
<p>Life cycle management is a third sustainability theme tackled by McDonough and Braungart’s Cradle-to-Cradle. They describe two component metabolisms from which products are formed: the biological cycle and the technical cycle, and they say that extending the useful life of a product is achieved by optimising products within these cycles. The biological metabolism consists of biological nutrients; materials that can be “consumed by microorganisms in the soil and by other animals”. Conversely, the technical metabolism is filled with technical nutrients that are man-made materials; industrial components. The authors suggest that we must keep these cycles entirely separate if we are to salvage and reuse components in new products. If we do not, we create products they describe as “monstrous hybrids”. McDonough and Braungart describe the modern leather shoe as a monstrous hybrid. Leather is, of course, a biological nutrient that would traditionally be coloured with a vegetable chemical: tannis. Recently, leather shoes have been coloured with chromium tanning, a technical nutrient, to avoid the energy-intensive process of extracting tannis from trees. Because these nutrients become intrinsically locked together, neither component can be salvaged at their end of use and their value is lost. I believe this analysis is extremely valuable in an engineering context as it requires that life cycle thinking is at the forefront of the designer’s mind at the earliest stage of a project, rather than an afterthought.</p>
<p>The authors use this analogy to comment on recycling, or a lack thereof, in industry more generally. They introduce the concept of “downcycling”, whereby valuable components are not reused in their original context, but downgraded to a rudimentary form; plastic water bottles are often melted down and moulded into speed bumps, for example. The authors tell the reader that separating the biological and technical metabolisms will in fact enable upcycling, whereby components can be used in applications better than their original use. What a wonderful idea! Components can be used again and again in ever-improving contexts. I struggle with this concept and find, once again, the boundaries of their system are conveniently limited. Thermodynamics is their recurring enemy, with the second law stating that everything tends toward disorder. Upcycling components to higher quality contexts, contexts of higher order, requires the input of energy – a distinct omission from their analysis. The recycling system becomes tricky when you consider the second law as all matter is freefalling into a cavernous well of disorder. In my opinion, we should strive for recycling in the same context rather than upcycling, but even this requires an energy input to maintain the same order or quality. I think it is unrealistic and uneconomic to expect that technical nutrients can be transformed to applications of higher quality – a practice that any corporate executive would endorse if he thought it were plausible – and believe it comes at the detriment to the authors otherwise effective observations on life cycle management.</p>
<p>Braungart and McDonough’s Cradle-to-Cradle is a seminal piece in the sustainability field that undoubtedly inspired the circularity movement and highlighted the wasteful consumption paradigm that plagues society. Their call for the reader to take inspiration from nature’s waste-free, abundant supply chains is effective as is their observations of the technical and biological metabolisms that contribute to our products. The author’s often utopic philosophies seem tricky to implement and their analyses appear limited, but their optimism about the future is refreshing in a world filled with negative rhetoric. A true strength of their message is the coherent language through which it is delivered, but I would argue that, despite the book’s accessibility, there are very few readers who can realistically enact the change they want. This power is inherently reserved for political leaders and captains of industry and their message offers few hand-holds for the general public to grip to. Nevertheless, Cradle-to-Cradle is an overwhelmingly positive contribution to sustainable thought that opens a dialogue with all stakeholders of the design process. If the cherry tree represents the ideal to which we aspire, Michael Braungart and William McDonough have planted the first seeds.</p>
<h2 id="references">References</h2>
<p>[1]        K. E. Boulding, “The Economics of the Coming Spaceship Earth,” Environ. Qual. Grow. Econ., pp. 3–14, Mar. 1966.</p>
<p>[2]        D. W. Pearce and R. Kerry Turner, Economics of Natural Resources and the Environment. 1989.</p>
<p>[3]        T. Jackson, Material Concerns: Pollution, profit and quality of life. Routledge, 1996.</p>
<p>[4]        C. L. Merrill, “Biomimicry of the Dioxygen Active Site In The Copper Proteins Hemocyanin and Cytochrome Oxidase,” Thesis, 1982.</p>
<p>[5]        J. Benyus, Biomimicry: Innovation Inspired by Nature. William Morrow &amp; Company, Inc., 1997.</p>
]]></content>
        </item>
        
        <item>
            <title>Plastic Politics</title>
            <link>https://enjeeneer.io/posts/2018/10/plastic-politics/</link>
            <pubDate>Fri, 19 Oct 2018 16:56:04 +0000</pubDate>
            
            <guid>https://enjeeneer.io/posts/2018/10/plastic-politics/</guid>
            <description>Single-use plastics and the European Single Market have more in common than you may think; they were both popularised in the middle of the 20th century, they both bring economic benefit to western economies and they both have a party trying to discontinue their use. Negotiating the terms of my exit from the plastics market (read: Plaxit) has required diplomacy and strong leadership in equal measure. Admittedly, I have not penned a Chequers plan, nor have I travelled to Brussels for negotiations, but I have had to make compromises.</description>
            <content type="html"><![CDATA[<figure><img src="/img/plastic_politics.png" width="350" height="300"/>
</figure>

<p>Single-use plastics and the European Single Market have more in common than you may think; they were both popularised in the middle of the 20th century, they both bring economic benefit to western economies and they both have a party trying to discontinue their use. Negotiating the terms of my exit from the plastics market (read: Plaxit) has required diplomacy and strong leadership in equal measure. Admittedly, I have not penned a Chequers plan, nor have I travelled to Brussels for negotiations, but I have had to make compromises. Sadly, the hard-Plaxit you readers have cried for is proving tricky to reconcile. Your brain may be filling with statements like: “Why are you even following through with this if you can’t do it properly?” or “You’ve fed us misinformation!” or “Why don’t you scrap the idea entirely?!” and all of these have validity. But we can’t go back now, that would be undemocratic. Or could we?</p>
<p>No we can’t. So I intend on playing the hand I’ve been dealt as best I can. The primary policy I am here to discuss relates to my use of University cafeterias. I am allowing myself to eat lunch, and occasionally dinner, in food halls provided the food I am eating is not observably housed in plastic at the point of eating. How’s that for policy design? Clearly, the issue is that the food on my plate, say the Lamb Tagine I had for dinner last night, will have been removed from plastic containment by chefs before it got to me. The Lamb may have come in some butcher’s paper (unlikely), but the cous-cous will have undoubtedly been poured from a plastic bag. I appreciate this is cheeky, but your author would like to eat something that isn’t loose vegetables for every meal – the modus operandi thus far. I should emphasise that other than this slight rule-bending, I am yet to buy any food or other goods delivered in plastic. I shall explain my progress to date.</p>
<p>I am practicing the obvious stuff that I’m sure many of you do too: carry a keep-cup for coffee, carry a steel water bottle, carry a cloth bag for any shopping and carry steel cutlery on the off-chance the only available cutlery is plastic. I have dealt with more subtle hurdles too, including: not eating crisps or chocolate, not eating meat, not eating pre-packed salads and not eating pre-made sandwiches or pastas for lunch. I have been primarily buying my food from the local farmer’s market (see below) as opposed to my nearest supermarket as said supermarkets are impossible to navigate without purchasing plastic – more on this in a future blog. Not only is the produce nicer than that of a supermarket, but there is of course benefit in supporting local farmers and businesses.</p>
<p>So, things are on-track (bar cafeteria-gate) and I plan on ramping up my change effort to affect those other than myself in the weeks to come. I hope you accept the requests outlined above, otherwise we will descend into an inevitable no-deal scenario and nobody wants that. Thanks for reading, I hope you stay tuned.</p>
<p>PS</p>
<p>As I was about to post this blog, there is <a href="https://www.bbc.co.uk/news/business-45911794">plastic-related news hot off the presses</a>. The Environment Agency have made accusations against UK recycling firms who are reportedly profiting illegally from recycling they aren’t even doing. As if you needed another reason to give up plastic entirely!</p>
]]></content>
        </item>
        
        <item>
            <title>Change</title>
            <link>https://enjeeneer.io/posts/2018/10/change/</link>
            <pubDate>Fri, 12 Oct 2018 16:55:47 +0000</pubDate>
            
            <guid>https://enjeeneer.io/posts/2018/10/change/</guid>
            <description>What’s that Mahatmu Gandhi quote again? The one about changing the world and so-on? It never really resonated with me. It’s rather pessimistic, but I often succumb to Down’s paradox or the paradox of voting, that is, the disbelief that one’s vote can alter the outcome of an election or, crucially, that one’s actions can cause a tangible disruption to our environment. During my first 10 days at the University of Cambridge I have quickly (and quite rightly) felt my pessimism is unshared.</description>
            <content type="html"><![CDATA[<p>What’s that Mahatmu Gandhi quote again? The one about changing the world and so-on? It never really resonated with me. It’s rather pessimistic, but I often succumb to Down’s paradox or the paradox of voting, that is, the disbelief that one’s vote can alter the outcome of an election or, crucially, that one’s actions can cause a tangible disruption to our environment. During my first 10 days at the University of Cambridge I have quickly (and quite rightly) felt my pessimism is unshared. Academics and students here work tirelessly to make the world a better place and there is no doubt that countless individuals will indeed disprove Down’s paradox and enact change that benefits us all. These same academics have set my cohort a challenge to change one thing in our lives for the duration of this term to understand the barriers to change, both personal and external. This is a timely test given the IPCC’s recent report detailing our accelerated impact on the climate and the risks our actions pose to sea level rises, coral reefs, fisheries and agricultural land. The need for change could not be more acute.</p>
<p>It’s funny, my initial reaction upon hearing of our Change Challenge was one of opposition. Upon reflection, I am not particularly akin to change, nor am I any good at it. Joining the university is arguably the biggest change I have made in the last 5 years and, hilariously, prior to that, my biggest change was attending the university where I studied my undergraduate degree. I am capable of making small changes quite readily: do a bit of running, read more, travel to some new places, but I sub-consciously resist wholesale change. In fairness, I think most of us resist huge change too – the status quo is warm, comfortable and known. Indeed, I feel change resistance is a significant part of our evolutionary makeup. Older generations regularly lament how the world has changed and how younger people don’t act how they would like. This is understandable as their actions, and the way society operated in their youth, allowed them to survive (dare I say thrive!) and in many cases allowed them great wealth and opportunity. But the way we do things now needs not only to change, but be turned on its head. We cannot keep treating our planet the way we do if we want other generations to enjoy what we enjoy. This is hard! I quite like the warm, comfortable status-quo that is our CO2-asphyxiated home. But if we are going to change it we need people like me who don’t like change to really change the way we operate.</p>
<p>So I’ve decided for my Change Challenge I am going to attempt to give up single-use plastic entirely. I have chosen this not just for the obvious ecological benefits, but to change the way I currently interact with plastic. I want to view my world through plastic-tinted glasses, if you will.  I know the issues associated with this, namely that all of western society revolves around the use of plastic, but that is primarily why I am doing it. I want to do something that is really going to affect my lifestyle to challenge my resistance to change. I am also aware that this is essentially impossible given our extensive use of plastic in food packaging and hygiene products, but I imagine my attempts will more clearly highlight where we excessively use plastic and identify the low-hanging fruit for plastic reduction in such sectors. I intend to update this blog bi-weekly to discuss the difficulties I am having and perhaps offer some strategies for reducing our plastic consumption without going to such extremes as these.</p>
<p>Be the change you want to see in the world he said. This time, Gandhi, I’ll try and listen.</p>
]]></content>
        </item>
        
    </channel>
</rss>
