<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Scott Jeen</title>
    <link>https://enjeeneer.io/</link>
    <description>Recent content on Scott Jeen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Fri, 12 Aug 2022 09:34:36 +0100</lastBuildDate><atom:link href="https://enjeeneer.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Low Emission Building Control with Zero-Shot Reinforcement Learning</title>
      <link>https://enjeeneer.io/projects/pearl/</link>
      <pubDate>Fri, 12 Aug 2022 09:34:36 +0100</pubDate>
      
      <guid>https://enjeeneer.io/projects/pearl/</guid>
      <description>Scott R. Jeen\(^{1,3}\), Alessandro Abate\(^{2,3}\), Jonathan M. Cullen\(^{1}\) \(^{1}\) University of Cambridge
\(^{2}\) University of Oxford
\(^{3}\) Alan Turing Institute
[Paper] [View Code on GitHub] [Talk]
   PEARL: Probabilistic Emission-Abating Reinforcement Learning  Presenting PEARL: Probabilistic Emission-Abating Reinforcement Learning, a deep RL algorithm that can find performant building control policies online, without pre-training&amp;ndash;the first time this has been shown to be possible. We show PEARL can reduce annual emissions by 31% when compared with a conventional controller whilst maintaining thermal comfort, and outperforms all RL baselines used for comparison.</description>
    </item>
    
    <item>
      <title>Zero Shot Building Control</title>
      <link>https://enjeeneer.io/talks/2022-06-23oxcav/</link>
      <pubDate>Thu, 23 Jun 2022 09:34:36 +0100</pubDate>
      
      <guid>https://enjeeneer.io/talks/2022-06-23oxcav/</guid>
      <description>Abstract
Heating and cooling systems in buildings account for 31% of global energy use, much of which are regulated by Rule Based Controllers (RBCs) that neither maximise energy efficiency nor minimise emissions by interacting optimally with the grid. Control via Reinforcement Learning (RL) has been shown to significantly improve building energy efficiency, but existing solutions require pre-training in simulators that are prohibitively expensive to obtain for every building in the world.</description>
    </item>
    
    <item>
      <title>One Hour RL</title>
      <link>https://enjeeneer.io/posts/2022/02/one-hour-rl/</link>
      <pubDate>Fri, 25 Feb 2022 15:23:20 +0000</pubDate>
      
      <guid>https://enjeeneer.io/posts/2022/02/one-hour-rl/</guid>
      <description>An Introduction to Reinforcement Learning Tom Bewley &amp;amp; Scott Jeen Alan Turing Institute 24/02/2022 The best way to walk through this tutorial is using the accompanying Jupyter Notebook:

[Jupyter Notebook]
1 | Markov Decision Processes: A Model of Sequential Decision Making 1.1. MDP (semi-)Formalism In reinforcement learning (RL), an agent takes actions in an environment to change its state over discrete timesteps $t$, with the goal of maximising the future sum of a scalar quantity known as reward.</description>
    </item>
    
    <item>
      <title>Presenting with Jupyter Notebooks</title>
      <link>https://enjeeneer.io/posts/2021/11/presenting-with-jupyter-notebooks/</link>
      <pubDate>Wed, 17 Nov 2021 09:03:11 -0500</pubDate>
      
      <guid>https://enjeeneer.io/posts/2021/11/presenting-with-jupyter-notebooks/</guid>
      <description>The best way to walk through this tutorial is using the accompanying Jupyter Notebook: 
[Jupyter Notebook]
- In the last year I&amp;rsquo;ve started presenting work using Jupyter Notebooks, rebelling against the Bill Gates&#39;-driven status-quo. Here I&amp;rsquo;ll explain how to do it. It&amp;rsquo;s not difficult, but in my opinion makes presentations look slicker, whilst allowing you to run code live in a presentation if you like. First, we need to download the plug-in that gives us the presentation functionality, it&amp;rsquo;s called RISE.</description>
    </item>
    
    <item>
      <title>Space Invasion, Space Invaders, and Climate Change Mitigation</title>
      <link>https://enjeeneer.io/talks/2021-06-03-jesusmcrgreen/</link>
      <pubDate>Thu, 03 Jun 2021 11:50:52 +0100</pubDate>
      
      <guid>https://enjeeneer.io/talks/2021-06-03-jesusmcrgreen/</guid>
      <description>Abstract
Recent advances in the field of Reinforcement Learning, a subfield of Artificial Intelligence, have shown computers can achieve superhuman performance at complex games like Go, Starcraft and the Atari Suite without human knowledge. What if, instead of playing Space Invaders, we play the game of climate change mitigation? More specifically, how can we design games where the goal is to minimise carbon emissions in some setting, and learn to play them optimally?</description>
    </item>
    
    <item>
      <title>Notes, Exercises and Code for Sutton and Barto&#39;s Reinforcement Learning: An Introduction (2018)</title>
      <link>https://enjeeneer.io/posts/2021/04/notes-exercises-and-code-for-sutton-and-bartos-reinforcement-learning-an-introduction-2018/</link>
      <pubDate>Fri, 30 Apr 2021 18:18:24 +0100</pubDate>
      
      <guid>https://enjeeneer.io/posts/2021/04/notes-exercises-and-code-for-sutton-and-bartos-reinforcement-learning-an-introduction-2018/</guid>
      <description>In the last few weeks I&amp;rsquo;ve been compiling a set of notes and exercise solutions for Sutton and Barto&amp;rsquo;s Reinforcement Learning: An Introduction. Admittedly, these were produced for my own benefit, but if you&amp;rsquo;d like to look at my notes, my (probably incorrect) answers to the exercises, or the code accommodating those answers, I&amp;rsquo;ll link directly to them below:
 Notes Exercises Code  Thanks to Bryn Hayder for inspiring this idea, and for providing his exercise solutions which helped me throughout.</description>
    </item>
    
    <item>
      <title>Scott&#39;s Uncomprehensive Guide to Scotland </title>
      <link>https://enjeeneer.io/posts/2021/04/scotts-uncomprehensive-guide-to-scotland/</link>
      <pubDate>Mon, 19 Apr 2021 21:08:05 +0000</pubDate>
      
      <guid>https://enjeeneer.io/posts/2021/04/scotts-uncomprehensive-guide-to-scotland/</guid>
      <description>Hello treasured friend. If you&amp;rsquo;re reading this, it&amp;rsquo;s probably because I&amp;rsquo;ve force-fed you a link after discussing your upcoming trip to Scotland. I hope this is useful to you in some way. Think of this a travel guide that you can dip into when you find yourself in one of these places either hungry or bored. I don&amp;rsquo;t describe anything in detail, you&amp;rsquo;ll just have to take me on my word that these places are worth visiting.</description>
    </item>
    
    <item>
      <title>Golf, attention-spans, and degrowth</title>
      <link>https://enjeeneer.io/posts/2021/04/golf-attention-spans-and-degrowth/</link>
      <pubDate>Wed, 14 Apr 2021 16:10:24 +0100</pubDate>
      
      <guid>https://enjeeneer.io/posts/2021/04/golf-attention-spans-and-degrowth/</guid>
      <description>&amp;ldquo;It&amp;rsquo;s downwind, if he throws everything at it he&amp;rsquo;s got a chance of making the front edge&amp;rdquo; said Nick Faldo, CBS commentator and 6 time major champion, as Bryson DeChambeau ambled to the 6th tee at Bay Hill Country Club 5 weeks ago. DeChambeau was leading Arnold Palmer&amp;rsquo;s Bay Hill Invitational and considering his options from the tee of the 528 yard par 5. I say 528 yards, as that is the distance as offered by Bay Hill&amp;rsquo;s scorecard, but that is the distance measured around the lake blocking the player&amp;rsquo;s line from tee to green, but were the player to bypass the boomerang arc and aim directly for the green, the hole plays only 340 yards.</description>
    </item>
    
    <item>
      <title>Taking a peek inside black-box function approximators</title>
      <link>https://enjeeneer.io/talks/2021-03-15-xchangeseminar/</link>
      <pubDate>Mon, 15 Mar 2021 16:55:47 +0000</pubDate>
      
      <guid>https://enjeeneer.io/talks/2021-03-15-xchangeseminar/</guid>
      <description>Abstract
In this talk, I will introduce deterministic and probabilistic black-box function approximators, with applications for timeseries modelling. First, I introduce the multilayer perceptron (neural network), demonstrating how they can model arbitrarily complex functions, but highlighting their propensity to overfit the training data. Second, I introduce the Gaussian process (GP), a bayesian, probabilistic approach to function approximation. I visualise how GP&amp;rsquo;s quantify uncertainty, a robust trait in the context of control, but highlight their inability to scale to large datasets.</description>
    </item>
    
    <item>
      <title>Projects</title>
      <link>https://enjeeneer.io/projects/</link>
      <pubDate>Mon, 04 Jan 2021 17:34:13 +0000</pubDate>
      
      <guid>https://enjeeneer.io/projects/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
